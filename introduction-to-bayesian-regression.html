<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Introduction to Bayesian Regression | An Introduction to Bayesian Thinking</title>
  <meta name="description" content="Chapter 6 Introduction to Bayesian Regression | An Introduction to Bayesian Thinking" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Introduction to Bayesian Regression | An Introduction to Bayesian Thinking" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="StatsWithR/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Introduction to Bayesian Regression | An Introduction to Bayesian Thinking" />
  
  
  

<meta name="author" content="Merlise Clyde" />
<meta name="author" content="Mine Cetinkaya-Rundel" />
<meta name="author" content="Colin Rundel" />
<meta name="author" content="David Banks" />
<meta name="author" content="Christine Chai" />
<meta name="author" content="Lizzy Huang" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-testing-with-normal-populations.html"/>
<link rel="next" href="bayesian-model-choice.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html"><i class="fa fa-check"></i><b>1</b> The Basics of Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i><b>1.1</b> Bayes’ Rule</a><ul>
<li class="chapter" data-level="1.1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:bayes-rule"><i class="fa fa-check"></i><b>1.1.1</b> Conditional Probabilities &amp; Bayes’ Rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:diagnostic-testing"><i class="fa fa-check"></i><b>1.1.2</b> Bayes’ Rule and Diagnostic Testing</a></li>
<li class="chapter" data-level="1.1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-updating"><i class="fa fa-check"></i><b>1.1.3</b> Bayes Updating</a></li>
<li class="chapter" data-level="1.1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayesian-vs.frequentist-definitions-of-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian vs. Frequentist Definitions of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion"><i class="fa fa-check"></i><b>1.2</b> Inference for a Proportion</a><ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-frequentist-approach"><i class="fa fa-check"></i><b>1.2.1</b> Inference for a Proportion: Frequentist Approach</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-bayesian-approach"><i class="fa fa-check"></i><b>1.2.2</b> Inference for a Proportion: Bayesian Approach</a></li>
<li class="chapter" data-level="1.2.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#effect-of-sample-size-on-the-posterior"><i class="fa fa-check"></i><b>1.2.3</b> Effect of Sample Size on the Posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.bayesian-inference"><i class="fa fa-check"></i><b>1.3</b> Frequentist vs. Bayesian Inference</a><ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.bayesian-inference-1"><i class="fa fa-check"></i><b>1.3.1</b> Frequentist vs. Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#continuous-variables-and-eliciting-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Continuous Variables and Eliciting Probability Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-the-discrete-to-the-continuous"><i class="fa fa-check"></i><b>2.1.1</b> From the Discrete to the Continuous</a></li>
<li class="chapter" data-level="2.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#elicitation"><i class="fa fa-check"></i><b>2.1.2</b> Elicitation</a></li>
<li class="chapter" data-level="2.1.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugacy"><i class="fa fa-check"></i><b>2.1.3</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#three-conjugate-families"><i class="fa fa-check"></i><b>2.2</b> Three Conjugate Families</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#inference-on-a-binomial-proportion"><i class="fa fa-check"></i><b>2.2.1</b> Inference on a Binomial Proportion</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-gamma-poisson-conjugate-families"><i class="fa fa-check"></i><b>2.2.2</b> The Gamma-Poisson Conjugate Families</a></li>
<li class="chapter" data-level="2.2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sec:normal-normal"><i class="fa fa-check"></i><b>2.2.3</b> The Normal-Normal Conjugate Families</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals-and-predictive-inference"><i class="fa fa-check"></i><b>2.3</b> Credible Intervals and Predictive Inference</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-conjugate-priors"><i class="fa fa-check"></i><b>2.3.1</b> Non-Conjugate Priors</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.3.2</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#predictive-inference"><i class="fa fa-check"></i><b>2.3.3</b> Predictive Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html"><i class="fa fa-check"></i><b>3</b> Losses and Decision Making</a><ul>
<li class="chapter" data-level="3.1" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#bayesian-decision-making"><i class="fa fa-check"></i><b>3.1</b> Bayesian Decision Making</a></li>
<li class="chapter" data-level="3.2" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#loss-functions"><i class="fa fa-check"></i><b>3.2</b> Loss Functions</a></li>
<li class="chapter" data-level="3.3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#working-with-loss-functions"><i class="fa fa-check"></i><b>3.3</b> Working with Loss Functions</a></li>
<li class="chapter" data-level="3.4" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#minimizing-expected-loss-for-hypothesis-testing"><i class="fa fa-check"></i><b>3.4</b> Minimizing Expected Loss for Hypothesis Testing</a></li>
<li class="chapter" data-level="3.5" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#sec:bayes-factors"><i class="fa fa-check"></i><b>3.5</b> Posterior Probabilities of Hypotheses and Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html"><i class="fa fa-check"></i><b>4</b> Inference and Decision-Making with Multiple Parameters</a><ul>
<li class="chapter" data-level="4.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:normal-gamma"><i class="fa fa-check"></i><b>4.1</b> The Normal-Gamma Conjugate Family</a><ul>
<li class="chapter" data-level="4.1.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-prior-for-mu-and-sigma2"><i class="fa fa-check"></i><b>4.1.1</b> Conjugate Prior for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="4.1.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-posterior-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Conjugate Posterior Distribution</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#marginal-distribution-for-mu-student-t"><i class="fa fa-check"></i><b>4.1.3</b> Marginal Distribution for <span class="math inline">\(\mu\)</span>: Student <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="4.1.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#credible-intervals-for-mu"><i class="fa fa-check"></i><b>4.1.4</b> Credible Intervals for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="4.1.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:tapwater"><i class="fa fa-check"></i><b>4.1.5</b> Example: TTHM in Tapwater</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#section-summary"><i class="fa fa-check"></i><b>4.1.6</b> Section Summary</a></li>
<li class="chapter" data-level="4.1.7" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#optional-derivations"><i class="fa fa-check"></i><b>4.1.7</b> (Optional) Derivations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MC"><i class="fa fa-check"></i><b>4.2</b> Monte Carlo Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Sampling</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#tap-water-example-continued"><i class="fa fa-check"></i><b>4.2.2</b> Tap Water Example (continued)</a></li>
<li class="chapter" data-level="4.2.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-inference-for-functions-of-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Monte Carlo Inference for Functions of Parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary"><i class="fa fa-check"></i><b>4.2.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-predictive"><i class="fa fa-check"></i><b>4.3</b> Predictive Distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#prior-predictive-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Prior Predictive Distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#tap-water-example-continued-1"><i class="fa fa-check"></i><b>4.3.2</b> Tap Water Example (continued)</a></li>
<li class="chapter" data-level="4.3.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sampling-from-the-prior-predictive-in-r"><i class="fa fa-check"></i><b>4.3.3</b> Sampling from the Prior Predictive in <code>R</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#posterior-predictive"><i class="fa fa-check"></i><b>4.3.4</b> Posterior Predictive</a></li>
<li class="chapter" data-level="4.3.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary-1"><i class="fa fa-check"></i><b>4.3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-reference"><i class="fa fa-check"></i><b>4.4</b> Reference Priors</a></li>
<li class="chapter" data-level="4.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-Cauchy"><i class="fa fa-check"></i><b>4.5</b> Mixtures of Conjugate Priors</a></li>
<li class="chapter" data-level="4.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MCMC"><i class="fa fa-check"></i><b>4.6</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Testing with Normal Populations</a><ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:known-var"><i class="fa fa-check"></i><b>5.1</b> Bayes Factors for Testing a Normal Mean: variance known</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#comparing-two-paired-means-using-bayes-factors"><i class="fa fa-check"></i><b>5.2</b> Comparing Two Paired Means using Bayes Factors</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:indep-means"><i class="fa fa-check"></i><b>5.3</b> Comparing Independent Means: Hypothesis Testing</a></li>
<li class="chapter" data-level="5.4" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#inference-after-testing"><i class="fa fa-check"></i><b>5.4</b> Inference after Testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html"><i class="fa fa-check"></i><b>6</b> Introduction to Bayesian Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:simple-linear"><i class="fa fa-check"></i><b>6.1</b> Bayesian Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#frequentist-ordinary-least-square-ols-simple-linear-regression"><i class="fa fa-check"></i><b>6.1.1</b> Frequentist Ordinary Least Square (OLS) Simple Linear Regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-simple-linear-regression-using-the-reference-prior"><i class="fa fa-check"></i><b>6.1.2</b> Bayesian Simple Linear Regression Using the Reference Prior</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:informative-prior"><i class="fa fa-check"></i><b>6.1.3</b> Informative Priors</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:derivations"><i class="fa fa-check"></i><b>6.1.4</b> (Optional) Derivations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-beta"><i class="fa fa-check"></i><b>6.1.5</b> Marginal Posterior Distribution of <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="6.1.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-alpha"><i class="fa fa-check"></i><b>6.1.6</b> Marginal Posterior Distribution of <span class="math inline">\(\alpha\)</span></a></li>
<li class="chapter" data-level="6.1.7" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-sigma2"><i class="fa fa-check"></i><b>6.1.7</b> Marginal Posterior Distribution of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.8" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#joint-normal-gamma-posterior-distributions"><i class="fa fa-check"></i><b>6.1.8</b> Joint Normal-Gamma Posterior Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Checking-outliers"><i class="fa fa-check"></i><b>6.2</b> Checking Outliers</a><ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-distribution-of-epsilon_j-conditioning-on-sigma2"><i class="fa fa-check"></i><b>6.2.1</b> Posterior Distribution of <span class="math inline">\(\epsilon_j\)</span> Conditioning On <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#implementation-using-bas-package"><i class="fa fa-check"></i><b>6.2.2</b> Implementation Using <code>BAS</code> Package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Bayes-multiple-regression"><i class="fa fa-check"></i><b>6.3</b> Bayesian Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.3.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#the-model"><i class="fa fa-check"></i><b>6.3.1</b> The Model</a></li>
<li class="chapter" data-level="6.3.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#data-pre-processing"><i class="fa fa-check"></i><b>6.3.2</b> Data Pre-processing</a></li>
<li class="chapter" data-level="6.3.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#specify-bayesian-prior-distributions"><i class="fa fa-check"></i><b>6.3.3</b> Specify Bayesian Prior Distributions</a></li>
<li class="chapter" data-level="6.3.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#fitting-the-bayesian-model"><i class="fa fa-check"></i><b>6.3.4</b> Fitting the Bayesian Model</a></li>
<li class="chapter" data-level="6.3.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-means-and-posterior-standard-deviations"><i class="fa fa-check"></i><b>6.3.5</b> Posterior Means and Posterior Standard Deviations</a></li>
<li class="chapter" data-level="6.3.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#credible-intervals-summary"><i class="fa fa-check"></i><b>6.3.6</b> Credible Intervals Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#summary-2"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html"><i class="fa fa-check"></i><b>7</b> Bayesian Model Choice</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BIC"><i class="fa fa-check"></i><b>7.1</b> Bayesian Information Criterion (BIC)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#definition-of-bic"><i class="fa fa-check"></i><b>7.1.1</b> Definition of BIC</a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#backward-elimination-with-bic"><i class="fa fa-check"></i><b>7.1.2</b> Backward Elimination with BIC</a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-estimates-under-reference-prior-for-best-bic-model"><i class="fa fa-check"></i><b>7.1.3</b> Coefficient Estimates Under Reference Prior for Best BIC Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#other-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Other Criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BMU"><i class="fa fa-check"></i><b>7.2</b> Bayesian Model Uncertainty</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#model-uncertainty"><i class="fa fa-check"></i><b>7.2.1</b> Model Uncertainty</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#calculating-posterior-probability-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Calculating Posterior Probability in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>7.3</b> Bayesian Model Averaging</a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#visualizing-model-uncertainty"><i class="fa fa-check"></i><b>7.3.1</b> Visualizing Model Uncertainty</a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging-using-posterior-probability"><i class="fa fa-check"></i><b>7.3.2</b> Bayesian Model Averaging Using Posterior Probability</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-summary-under-bma"><i class="fa fa-check"></i><b>7.3.3</b> Coefficient Summary under BMA</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#summary-3"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html"><i class="fa fa-check"></i><b>8</b> Stochastic Explorations Using MCMC</a><ul>
<li class="chapter" data-level="8.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#stochastic-exploration"><i class="fa fa-check"></i><b>8.1</b> Stochastic Exploration</a><ul>
<li class="chapter" data-level="8.1.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#markov-chain-monte-carlo-exploration"><i class="fa fa-check"></i><b>8.1.1</b> Markov Chain Monte Carlo Exploration</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#other-priors-for-bayesian-model-uncertainty"><i class="fa fa-check"></i><b>8.2</b> Other Priors for Bayesian Model Uncertainty</a><ul>
<li class="chapter" data-level="8.2.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#zellners-g-prior"><i class="fa fa-check"></i><b>8.2.1</b> Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayes-factor-of-zellners-g-prior"><i class="fa fa-check"></i><b>8.2.2</b> Bayes Factor of Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#kids-cognitive-score-example"><i class="fa fa-check"></i><b>8.2.3</b> Kid’s Cognitive Score Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#r-demo-on-bas-package"><i class="fa fa-check"></i><b>8.3</b> R Demo on <code>BAS</code> Package</a><ul>
<li class="chapter" data-level="8.3.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#the-uscrime-data-set-and-data-processing"><i class="fa fa-check"></i><b>8.3.1</b> The <code>UScrime</code> Data Set and Data Processing</a></li>
<li class="chapter" data-level="8.3.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayesian-models-and-diagnostics"><i class="fa fa-check"></i><b>8.3.2</b> Bayesian Models and Diagnostics</a></li>
<li class="chapter" data-level="8.3.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#posterior-uncertainty-in-coefficients"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Uncertainty in Coefficients</a></li>
<li class="chapter" data-level="8.3.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction"><i class="fa fa-check"></i><b>8.3.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#decision-making-under-model-uncertainty"><i class="fa fa-check"></i><b>8.4</b> Decision Making Under Model Uncertainty</a><ul>
<li class="chapter" data-level="8.4.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#model-choice"><i class="fa fa-check"></i><b>8.4.1</b> Model Choice</a></li>
<li class="chapter" data-level="8.4.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction-with-new-data"><i class="fa fa-check"></i><b>8.4.2</b> Prediction with New Data</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#summary-4"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Thinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-bayesian-regression" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Introduction to Bayesian Regression</h1>
<p>In the previous chapter, we introduced Bayesian decision making using posterior probabilities and a variety of loss functions. We discussed how to minimize the expected loss for hypothesis testing. Moreover, we instroduced the concept of Bayes factors and gave some examples on how Bayes factors can be used in Bayesian hypothesis testing for comparison of two means. We also discussed how to choose appropriate and robust priors. When there is no conjugacy, we applied Markov Chain Monte Carlo simulation to approximate the posterior distributions of parameters of interest.</p>
<p>In this chapter, we will apply Bayesian inference methods to linear regression. We will first apply Bayesian statistics to simple linear regression models, then generalize the results to multiple linear regression models. We will see when using the reference prior, the posterior means, posterior standard deviations, and credible intervals of the coefficients coincide with the counterparts in the frequentist ordinary least square (OLS) linear regression models. However, using the Bayesian framework, we can now interpret credible intervals as the probabilities of the coefficients lying in such intervals.</p>

<div id="sec:simple-linear" class="section level2">
<h2><span class="header-section-number">6.1</span> Bayesian Simple Linear Regression</h2>
<p>In this section, we will turn to Bayesian inference in simple linear regressions. We will use the reference prior distribution on coefficients, which will provide a connection between the frequentist solutions and Bayesian answers. This provides a baseline analysis for comparions with more informative prior distributions. To illustrate the ideas, we will use an example of predicting body fat.</p>
<div id="frequentist-ordinary-least-square-ols-simple-linear-regression" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Frequentist Ordinary Least Square (OLS) Simple Linear Regression</h3>
<p>Obtaining accurate measurements of body fat is expensive and not easy to be done. Instead, predictive models that predict the percentage of body fat which use readily available measurements such as abdominal circumference are easy to use and inexpensive. We will apply a simple linear regression to predict body fat using abdominal circumference as an example to illustrate the Bayesian approach of linear regression. The data set <code>bodyfat</code> can be found from the library <code>BAS</code>.</p>
<p>To start, we load the <code>BAS</code> library (which can be downloaded from CRAN) to access the dataframe. We print out a summary of the variables in this dataframe.</p>

<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1"><span class="kw">library</span>(BAS)</a>
<a class="sourceLine" id="cb56-2" data-line-number="2"><span class="kw">data</span>(bodyfat)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3"><span class="kw">summary</span>(bodyfat)</a></code></pre></div>
<pre><code>##     Density         Bodyfat           Age            Weight     
##  Min.   :0.995   Min.   : 0.00   Min.   :22.00   Min.   :118.5  
##  1st Qu.:1.041   1st Qu.:12.47   1st Qu.:35.75   1st Qu.:159.0  
##  Median :1.055   Median :19.20   Median :43.00   Median :176.5  
##  Mean   :1.056   Mean   :19.15   Mean   :44.88   Mean   :178.9  
##  3rd Qu.:1.070   3rd Qu.:25.30   3rd Qu.:54.00   3rd Qu.:197.0  
##  Max.   :1.109   Max.   :47.50   Max.   :81.00   Max.   :363.1  
##      Height           Neck           Chest           Abdomen      
##  Min.   :29.50   Min.   :31.10   Min.   : 79.30   Min.   : 69.40  
##  1st Qu.:68.25   1st Qu.:36.40   1st Qu.: 94.35   1st Qu.: 84.58  
##  Median :70.00   Median :38.00   Median : 99.65   Median : 90.95  
##  Mean   :70.15   Mean   :37.99   Mean   :100.82   Mean   : 92.56  
##  3rd Qu.:72.25   3rd Qu.:39.42   3rd Qu.:105.38   3rd Qu.: 99.33  
##  Max.   :77.75   Max.   :51.20   Max.   :136.20   Max.   :148.10  
##       Hip            Thigh            Knee           Ankle          Biceps     
##  Min.   : 85.0   Min.   :47.20   Min.   :33.00   Min.   :19.1   Min.   :24.80  
##  1st Qu.: 95.5   1st Qu.:56.00   1st Qu.:36.98   1st Qu.:22.0   1st Qu.:30.20  
##  Median : 99.3   Median :59.00   Median :38.50   Median :22.8   Median :32.05  
##  Mean   : 99.9   Mean   :59.41   Mean   :38.59   Mean   :23.1   Mean   :32.27  
##  3rd Qu.:103.5   3rd Qu.:62.35   3rd Qu.:39.92   3rd Qu.:24.0   3rd Qu.:34.33  
##  Max.   :147.7   Max.   :87.30   Max.   :49.10   Max.   :33.9   Max.   :45.00  
##     Forearm          Wrist      
##  Min.   :21.00   Min.   :15.80  
##  1st Qu.:27.30   1st Qu.:17.60  
##  Median :28.70   Median :18.30  
##  Mean   :28.66   Mean   :18.23  
##  3rd Qu.:30.00   3rd Qu.:18.80  
##  Max.   :34.90   Max.   :21.40</code></pre>
<p></br></p>
<p>This data frame includes 252 observations of men’s body fat and other measurements, such as waist circumference (<code>Abdomen</code>). We will construct a Bayesian model of simple linear regression, which uses <code>Abdomen</code> to predict the response variable <code>Bodyfat</code>. Let <span class="math inline">\(y_i,\ i=1,\cdots, 252\)</span> denote the measurements of the response variable <code>Bodyfat</code>, and let <span class="math inline">\(x_i\)</span> be the waist circumference measurements <code>Abdomen</code>. We regress <code>Bodyfat</code> on the predictor <code>Abdomen</code>. This regression model can be formulated as
<span class="math display">\[ y_i = \alpha + \beta x_i + \epsilon_i, \quad i = 1,\cdots, 252.\]</span>
Here, we assume error <span class="math inline">\(\epsilon_i\)</span> is independent and identically distributed as normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[ \epsilon_i \mathrel{\mathop{\sim}\limits^{\rm iid}}\textsf{Normal}(0, \sigma^2). \]</span></p>
<p>The figure below shows the percentage body fat obtained from under water weighing and the abdominal circumference measurements for 252 men. To predict body fat, the line overlayed on the scatter plot illustrates the best fitting ordinary least squares (OLS) line obtained with the <code>lm</code> function in R.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1"><span class="co"># Frequentist OLS linear regression</span></a>
<a class="sourceLine" id="cb58-2" data-line-number="2">bodyfat.lm =<span class="st"> </span><span class="kw">lm</span>(Bodyfat <span class="op">~</span><span class="st"> </span>Abdomen, <span class="dt">data =</span> bodyfat)</a>
<a class="sourceLine" id="cb58-3" data-line-number="3"><span class="kw">summary</span>(bodyfat.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Bodyfat ~ Abdomen, data = bodyfat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.0160  -3.7557   0.0554   3.4215  12.9007 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -39.28018    2.66034  -14.77   &lt;2e-16 ***
## Abdomen       0.63130    0.02855   22.11   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.877 on 250 degrees of freedom
## Multiple R-squared:  0.6617, Adjusted R-squared:  0.6603 
## F-statistic: 488.9 on 1 and 250 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="co"># Extract coefficients</span></a>
<a class="sourceLine" id="cb60-2" data-line-number="2">beta =<span class="st"> </span><span class="kw">coef</span>(bodyfat.lm)</a>
<a class="sourceLine" id="cb60-3" data-line-number="3"></a>
<a class="sourceLine" id="cb60-4" data-line-number="4"><span class="co"># Visualize regression line on the scatter plot</span></a>
<a class="sourceLine" id="cb60-5" data-line-number="5"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb60-6" data-line-number="6"><span class="kw">ggplot</span>(<span class="dt">data =</span> bodyfat, <span class="kw">aes</span>(<span class="dt">x =</span> Abdomen, <span class="dt">y =</span> Bodyfat)) <span class="op">+</span></a>
<a class="sourceLine" id="cb60-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb60-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> beta[<span class="dv">1</span>], <span class="dt">slope =</span> beta[<span class="dv">2</span>], <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb60-9" data-line-number="9"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;abdomen circumference (cm)&quot;</span>) </a></code></pre></div>
<p><img src="06-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From the summary, we see that this model has an estimated slope, <span class="math inline">\(\hat{\beta}\)</span>, of 0.63 and an estimated <span class="math inline">\(y\)</span>-intercept, <span class="math inline">\(\hat{\alpha}\)</span>, of about -39.28%. This gives us the prediction formula
<span class="math display">\[ \widehat{\text{Bodyfat}} = -39.28 + 0.63\times\text{Abdomen}. \]</span>
For every additional centimeter, we expect body fat to increase by 0.63%. The negative <span class="math inline">\(y\)</span>-intercept of course does not make sense as a physical model, but neither does predicting a male with a waist of zero centimeter. Nevertheless, this linear regression may be an accurate approximation for prediction purpose for measurements that are in the observed range for this population.</p>
<p>Each of the residuals, which provide an estimate of the fitting error, is equal to <span class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i\)</span>, the difference between the observed value <span class="math inline">\(y_i\)</span> and the fited value <span class="math inline">\(\hat{y}_i = \hat{\alpha} + \hat{\beta}x_i\)</span>, where <span class="math inline">\(x_i\)</span> is the abdominal circumference for the <span class="math inline">\(i\)</span>th male. <span class="math inline">\(\hat{\epsilon}_i\)</span> is used for diagnostics as well as estimating the constant variance in the assumption of the model <span class="math inline">\(\sigma^2\)</span> via the mean squared error (MSE):
<span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n-2}\sum_i^n (y_i-\hat{y}_i)^2 = \frac{1}{n-2}\sum_i^n \hat{\epsilon}_i^2. \]</span>
Here the degrees of freedom <span class="math inline">\(n-2\)</span> are the number of observations adjusted for the number of parameters (which is 2) that we estimated in the regression. The MSE, <span class="math inline">\(\hat{\sigma}^2\)</span>, may be calculated through squaring the residuals of the output of <code>bodyfat.lm</code>.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="co"># Obtain residuals and n</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2">resid =<span class="st"> </span><span class="kw">residuals</span>(bodyfat.lm)</a>
<a class="sourceLine" id="cb61-3" data-line-number="3">n =<span class="st"> </span><span class="kw">length</span>(resid)</a>
<a class="sourceLine" id="cb61-4" data-line-number="4"></a>
<a class="sourceLine" id="cb61-5" data-line-number="5"><span class="co"># Calculate MSE</span></a>
<a class="sourceLine" id="cb61-6" data-line-number="6">MSE =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((resid <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb61-7" data-line-number="7">MSE</a></code></pre></div>
<pre><code>## [1] 23.78985</code></pre>
<p>If this model is correct, the residuals and fitted values should be uncorrelated, and the expected value of the residuals is zero. We apply the scatterplot of residuals versus fitted values, which provides an additional visual check of the model adequacy.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="co"># Combine residuals and fitted values into a data frame</span></a>
<a class="sourceLine" id="cb63-2" data-line-number="2">result =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fitted_values =</span> <span class="kw">fitted.values</span>(bodyfat.lm),</a>
<a class="sourceLine" id="cb63-3" data-line-number="3">                    <span class="dt">residuals =</span> <span class="kw">residuals</span>(bodyfat.lm))</a>
<a class="sourceLine" id="cb63-4" data-line-number="4"></a>
<a class="sourceLine" id="cb63-5" data-line-number="5"><span class="co"># Load library and plot residuals versus fitted values</span></a>
<a class="sourceLine" id="cb63-6" data-line-number="6"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb63-7" data-line-number="7"><span class="kw">ggplot</span>(<span class="dt">data =</span> result, <span class="kw">aes</span>(<span class="dt">x =</span> fitted_values, <span class="dt">y =</span> residuals)) <span class="op">+</span></a>
<a class="sourceLine" id="cb63-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">pch =</span> <span class="dv">1</span>, <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">0</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-10" data-line-number="10"><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;fitted value &quot;</span>, <span class="kw">widehat</span>(Bodyfat)))) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-11" data-line-number="11"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;residuals&quot;</span>)</a></code></pre></div>
<p><img src="06-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1"><span class="co"># Readers may also use `plot` function</span></a></code></pre></div>
<p>With the exception of one observation for the individual with the largest fitted value, the residual plot suggests that this linear regression is a reasonable approximation. The case number of the observation with the largest fitted value can be obtained using the <code>which</code> function in R. Further examination of the data frame shows that this case also has the largest waist measurement <code>Abdomen</code>. This may be our potential outlier and we will have more discussion on outlier in Section <a href="introduction-to-bayesian-regression.html#sec:Checking-outliers">6.2</a>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="co"># Find the observation with the largest fitted value</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2"><span class="kw">which.max</span>(<span class="kw">as.vector</span>(<span class="kw">fitted.values</span>(bodyfat.lm)))</a></code></pre></div>
<pre><code>## [1] 39</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="co"># Shows this observation has the largest Abdomen</span></a>
<a class="sourceLine" id="cb67-2" data-line-number="2"><span class="kw">which.max</span>(bodyfat<span class="op">$</span>Abdomen)</a></code></pre></div>
<pre><code>## [1] 39</code></pre>
<p>Furthermore, we can check the normal probability plot of the residuals for the assumption of normally distributed errors. We see that only Case 39, the one with the largest waist measurement, is exceptionally away from the normal quantile.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1"><span class="kw">plot</span>(bodyfat.lm, <span class="dt">which =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="06-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The confidence interval of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> can be constructed using the standard errors <span class="math inline">\(\text{se}_{\alpha}\)</span> and <span class="math inline">\(\text{se}_{\beta}\)</span> respectively. To proceed, we introduce notations of some “sums of squares”
<span class="math display">\[
\begin{aligned}
\text{S}_{xx} = &amp; \sum_i^n (x_i-\bar{x})^2\\
\text{S}_{yy} = &amp; \sum_i^n (y_i-\bar{x})^2 \\
\text{S}_{xy} = &amp; \sum_i^n (x_i-\bar{x})(y_i-\bar{y}) \\
\text{SSE}    = &amp; \sum_i^n (y_i-\hat{y}_i)^2 = \sum_i^n \hat{\epsilon}_i^2. 
\end{aligned}
\]</span></p>
<p>The estimates of the <span class="math inline">\(y\)</span>-intercept <span class="math inline">\(\alpha\)</span>, and the slope <span class="math inline">\(\beta\)</span>, which are denoted as <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> respectively, can be calculated using these “sums of squares”
<span class="math display">\[ \hat{\beta} = \frac{\sum_i (x_i-\bar{x})(y_i-\bar{y})}{\sum_i (x_i-\bar{x})^2} = \frac{\text{S}_{xy}}{\text{S}_{xx}},\qquad \qquad \hat{\alpha} = \bar{y} - \hat{\beta}\bar{x} = \bar{y}-\frac{\text{S}_{xy}}{\text{S}_{xx}}\bar{x}. \]</span></p>
<p>The last “sum of square” is the <em>sum of squares of errors</em> (SSE). Its sample mean is exactly the mean squared error (MSE) we introduced previously
<span class="math display">\[
\hat{\sigma}^2 = \frac{\text{SSE}}{n-2} = \text{MSE}.
\]</span></p>
<p>The standard errors, <span class="math inline">\(\text{se}_{\alpha}\)</span> and <span class="math inline">\(\text{se}_{\beta}\)</span>, are given as
<span class="math display">\[ 
\begin{aligned}
\text{se}_{\alpha} = &amp;  \sqrt{\frac{\text{SSE}}{n-2}\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right)} = \hat{\sigma}\sqrt{\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}},\\
\text{se}_{\beta} = &amp; \sqrt{\frac{\text{SSE}}{n-2}\frac{1}{\text{S}_{xx}}} = \frac{\hat{\sigma}}{\sqrt{\text{S}_{xx}}}.
\end{aligned}
\]</span></p>
<p>We may construct the confidence intervals of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> using the <span class="math inline">\(t\)</span>-statistics
<span class="math display">\[ 
t_\alpha^\ast = \frac{\alpha - \hat{\alpha}}{\text{se}_{\alpha}},\qquad \qquad t_\beta^\ast = \frac{\beta-\hat{\beta}}{\text{se}_{\beta}}.
\]</span></p>
<p>They both have degrees of freedom <span class="math inline">\(n-2\)</span>.</p>
</div>
<div id="bayesian-simple-linear-regression-using-the-reference-prior" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Bayesian Simple Linear Regression Using the Reference Prior</h3>
<p>Let us now turn to the Bayesian version and show that under the reference prior, we will obtain the posterior distributions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> analogous with the frequentist OLS results.</p>
<p>The Bayesian model starts with the same model as the classical frequentist approach:
<span class="math display">\[ y_i = \alpha + \beta x_i + \epsilon_i,\quad i = 1,\cdots, n. \]</span>
with the assumption that the errors, <span class="math inline">\(\epsilon_i\)</span>, are independent and identically distributed as normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>. This assumption is exactly the same as in the classical inference case for testing and constructing confidence intervals for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>Our goal is to update the distributions of the unknown parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>, based on the data <span class="math inline">\(x_1, y_1, \cdots, x_n, y_n\)</span>, where <span class="math inline">\(n\)</span> is the number of observations.</p>
<p>Under the assumption that the errors <span class="math inline">\(\epsilon_i\)</span> are normally distributed with constant variance <span class="math inline">\(\sigma^2\)</span>, we have for the random variable of each response <span class="math inline">\(Y_i\)</span>, conditioning on the observed data <span class="math inline">\(x_i\)</span> and the parameters <span class="math inline">\(\alpha,\ \beta,\ \sigma^2\)</span>, is normally distributed:
<span class="math display">\[ Y_i~|~x_i, \alpha, \beta,\sigma^2~ \sim~ \textsf{Normal}(\alpha + \beta x_i, \sigma^2),\qquad i = 1,\cdots, n. \]</span></p>
<p>That is, the likelihood of each <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(x_i, \alpha, \beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> is given by
<span class="math display">\[ p(y_i~|~x_i, \alpha, \beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i-(\alpha+\beta  x_i))^2}{2\sigma^2}\right). \]</span></p>
<p>The likelihood of <span class="math inline">\(Y_1,\cdots,Y_n\)</span> is the product of each likelihood <span class="math inline">\(p(y_i~|~x_i, \alpha, \beta,\sigma^2)\)</span>, since we assume each response <span class="math inline">\(Y_i\)</span> is independent from each other. Since this likelihood depends on the values of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>, it is sometimes denoted as a function of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\mathcal{L}(\alpha, \beta, \sigma^2)\)</span>.</p>
<p>We first consider the case under the reference prior, which is our standard noninformative prior. Using the reference prior, we will obtain familiar distributions as the posterior distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>, which gives the analogue to the frequentist results. Here we assume the joint prior distribution of <span class="math inline">\(\alpha,\ \beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> to be proportional to the inverse of <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display" id="eq:joint-prior">\[\begin{equation} 
p(\alpha, \beta, \sigma^2)\propto \frac{1}{\sigma^2}.
\tag{6.1}
\end{equation}\]</span></p>
<p>Using the hierachical model framework, this is equivalent to assuming that the joint prior distribution of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> under <span class="math inline">\(\sigma^2\)</span> is the uniform prior, while the prior distribution of <span class="math inline">\(\sigma^2\)</span> is proportional to <span class="math inline">\(\displaystyle \frac{1}{\sigma^2}\)</span>. That is
<span class="math display">\[ p(\alpha, \beta~|~\sigma^2) \propto 1, \qquad\qquad p(\sigma^2) \propto \frac{1}{\sigma^2}, \]</span>
Combining the two using conditional probability, we will get the same joint prior distribution <a href="introduction-to-bayesian-regression.html#eq:joint-prior">(6.1)</a>.</p>
<p>Then we apply the Bayes’ rule to derive the joint posterior distribution after observing data <span class="math inline">\(y_1,\cdots, y_n\)</span>. Bayes’ rule states that the joint posterior distribution of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> is proportional to the product of the likelihood and the joint prior distribution:
<span class="math display">\[
\begin{aligned}
p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots,y_n) \propto &amp; \left[\prod_i^n p(y_i~|~x_i,\alpha,\beta,\sigma^2)\right]p(\alpha, \beta,\sigma^2) \\
\propto &amp; \left[\left(\frac{1}{(\sigma^2)^{1/2}}\exp\left(-\frac{(y_1-(\alpha+\beta x_1 ))^2}{2\sigma^2}\right)\right)\times\cdots \right.\\
&amp; \left. \times \left(\frac{1}{(\sigma^2)^{1/2}}\exp\left(-\frac{(y_n-(\alpha +\beta x_n))^2}{2\sigma^2}\right)\right)\right]\times\left(\frac{1}{\sigma^2}\right)\\
\propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\sum_i\left(y_i-\alpha-\beta  x_i\right)^2}{2\sigma^2}\right)
\end{aligned}
\]</span></p>
<p>To obtain the marginal posterior distribution of <span class="math inline">\(\beta\)</span>, we need to integrate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma^2\)</span> out from the joint posterior distribution
<span class="math display">\[ p^*(\beta~|~y_1,\cdots,y_n) = \int_0^\infty \left(\int_{-\infty}^\infty p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots, y_n)\, d\alpha\right)\, d\sigma^2. \]</span></p>
<p>We leave the detailed calculation in Section <a href="introduction-to-bayesian-regression.html#sec:derivations">6.1.4</a>. It can be shown that the marginal posterior distribution of <span class="math inline">\(\beta\)</span> is the Student’s <span class="math inline">\(t\)</span>-distribution
<span class="math display">\[ \beta~|~y_1,\cdots,y_n ~\sim~ \textsf{t}\left(n-2,\ \hat{\beta},\ \frac{\hat{\sigma}^2}{\text{S}_{xx}}\right) = \textsf{t}\left(n-2,\  \hat{\beta},\ (\text{se}_{\beta})^2\right), \]</span>
with degrees of freedom <span class="math inline">\(n-2\)</span>, center at <span class="math inline">\(\hat{\beta}\)</span>, the slope estimate we obtained from the frequentist OLS model, and scale parameter <span class="math inline">\(\displaystyle \frac{\hat{\sigma}^2}{\text{S}_{xx}}=\left(\text{se}_{\beta}\right)^2\)</span>, which is the square of the standard error of <span class="math inline">\(\hat{\beta}\)</span> under the frequentist OLS model.</p>
<p>Similarly, we can integrate out <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> from the joint posterior distribution to get the marginal posterior distribution of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(p^*(\alpha~|~y_1,\cdots, y_n)\)</span>. It turns out that <span class="math inline">\(p^*(\alpha~|~y_1,\cdots,y_n)\)</span> is again a Student’s <span class="math inline">\(t\)</span>-distribution with degrees of freedom <span class="math inline">\(n-2\)</span>, center at <span class="math inline">\(\hat{\alpha}\)</span>, the <span class="math inline">\(y\)</span>-intercept estimate from the frequentist OLS model, and scale parameter <span class="math inline">\(\displaystyle \hat{\sigma}^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right) = \left(\text{se}_{\alpha}\right)^2\)</span>, which is the square of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> under the frequentist OLS model
<span class="math display">\[ \alpha~|~y_1,\cdots,y_n~\sim~  \textsf{t}\left(n-2,\ \hat{\alpha},\ \hat{\sigma}^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right)\right) = \textsf{t}\left(n-2,\ \hat{\alpha},\ (\text{se}_{\alpha})^2\right).\]</span></p>
<p>Finally, we can show that the marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> is the inverse Gamma distribution, or equivalently, the reciprocal of <span class="math inline">\(\sigma^2\)</span>, which is the precision <span class="math inline">\(\phi\)</span>, follows the Gamme distribution
<span class="math display">\[ \phi = \frac{1}{\sigma^2}~|~y_1,\cdots,y_n \sim \textsf{Gamma}\left(\frac{n-2}{2}, \frac{\text{SSE}}{2}\right). \]</span></p>
<p>Moreover, similar to the Normal-Gamma conjugacy under the reference prior introduced in the previous chapters, the joint posterior distribution of <span class="math inline">\(\beta, \sigma^2\)</span>, and the joint posterior distribution of <span class="math inline">\(\alpha, \sigma^2\)</span> are both Normal-Gamma. In particular, the posterior distribution of <span class="math inline">\(\beta\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[ \beta~|~\sigma^2, \text{data}~\sim ~\textsf{Normal}\left(\hat{\beta}, \frac{\sigma^2}{\text{S}_{xx}}\right), \]</span></p>
<p>and the posterior distribution of <span class="math inline">\(\alpha\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[ \alpha~|~\sigma^2, \text{data}~\sim ~\textsf{Normal}\left(\hat{\alpha}, \sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right)\right).\]</span></p>
<p><strong>Credible Intervals for Slope <span class="math inline">\(\beta\)</span> and <span class="math inline">\(y\)</span>-Intercept <span class="math inline">\(\alpha\)</span> </strong></p>
<p>The Bayesian posterior distribution results of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> show that under the reference prior, the posterior credible intervals are in fact <strong>numerically equivalent</strong> to the confidence intervals from the classical frequentist OLS analysis. This provides a baseline analysis for other Bayesian analyses with other informative prior distributions or perhaps other “objective” prior distributions, such as the Cauchy distribution. (Cauchy distribution is the Student’s <span class="math inline">\(t\)</span> prior with 1 degree of freedom.)</p>
<p>Since the credible intervals are numerically the same as the confidence intervals, We can use the <code>lm</code> function to obtain the OLS estimates and construct the credible intervals of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1">output =<span class="st"> </span><span class="kw">summary</span>(bodyfat.lm)<span class="op">$</span>coef[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</a>
<a class="sourceLine" id="cb70-2" data-line-number="2">output</a></code></pre></div>
<pre><code>##                Estimate Std. Error
## (Intercept) -39.2801847 2.66033696
## Abdomen       0.6313044 0.02855067</code></pre>
<p>The columns labeled <code>Estimate</code> and <code>Std. Error</code> are equivalent to the centers (or posterior means) and scale parameters (or standard deviations) in the two Student’s <span class="math inline">\(t\)</span>-distributions respectively. The credible intervals of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the same as the frequentist confidence intervals, but now we can interpret them from the Bayesian perspective.</p>
<p>The <code>confint</code> function provides 95% confidence intervals. Under the reference prior, they are equivalent to the 95% credible intervals. The code below extracts them and relabels the output as the Bayesian results.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1">out =<span class="st"> </span><span class="kw">cbind</span>(output, <span class="kw">confint</span>(bodyfat.lm))</a>
<a class="sourceLine" id="cb72-2" data-line-number="2"><span class="kw">colnames</span>(out) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;posterior mean&quot;</span>, <span class="st">&quot;posterior std&quot;</span>, <span class="st">&quot;2.5&quot;</span>, <span class="st">&quot;97.5&quot;</span>)</a>
<a class="sourceLine" id="cb72-3" data-line-number="3"><span class="kw">round</span>(out, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##             posterior mean posterior std    2.5   97.5
## (Intercept)         -39.28          2.66 -44.52 -34.04
## Abdomen               0.63          0.03   0.58   0.69</code></pre>
<p>These intervals coincide with the confidence intervals from the frequentist approach. The primary difference is the interpretation. For example, based on the data, we believe that there is 95% chance that body fat will increase by 5.75% up to 6.88% for every additional 10 centimeter increase in the waist circumference.</p>
<p><strong>Credible Intervals for the Mean <span class="math inline">\(\mu_Y\)</span> and the Prediction <span class="math inline">\(y_{n+1}\)</span></strong></p>
<p>From our assumption of the model
<span class="math display">\[ y_i = \alpha + \beta x_i + \epsilon_i, \]</span>
the mean of the response variable <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mu_Y\)</span>, at the point <span class="math inline">\(x_i\)</span> is
<span class="math display">\[ \mu_Y~|~x_i = E[Y~|~x_i] = \alpha + \beta x_i. \]</span></p>
<p>Under the reference prior, <span class="math inline">\(\mu_Y\)</span> has a posterior distributuion
<span class="math display">\[ 
\alpha + \beta x_i ~|~ \text{data} \sim \textsf{t}(n-2,\ \hat{\alpha} + \hat{\beta} x_i,\ \text{S}_{Y|X_i}^2), 
\]</span>
where
<span class="math display">\[
\text{S}_{Y|X_i}^2 = \hat{\sigma}^2\left(\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\text{S}_{xx}}\right)
\]</span></p>
<p>Any new prediction <span class="math inline">\(y_{n+1}\)</span> at a point <span class="math inline">\(x_{n+1}\)</span> also follows the Student’s <span class="math inline">\(t\)</span>-distribution
<span class="math display">\[ 
y_{n+1}~|~\text{data}, x_{n+1}\ \sim \textsf{t}\left(n-2,\  \hat{\alpha}+\hat{\beta} x_{n+1},\ \text{S}_{Y|X_{n+1}}^2\right), 
\]</span></p>
<p>where
<span class="math display">\[ 
\text{S}_{Y|X_{n+1}}^2 =\hat{\sigma}^2+\hat{\sigma}^2\left(\frac{1}{n}+\frac{(x_{n+1}-\bar{x})^2}{\text{S}_{xx}}\right) = \hat{\sigma}^2\left(1+\frac{1}{n}+\frac{(x_{n+1}-\bar{x})^2}{\text{S}_{xx}}\right).
\]</span></p>
<p>The variance for predicting a new observation <span class="math inline">\(y_{n+1}\)</span> has an extra <span class="math inline">\(\hat{\sigma}^2\)</span> which comes from the uncertainty of a new observation about the mean <span class="math inline">\(\mu_Y\)</span> estimated by the regression line.</p>
<p>We can extract these intervals using the <code>predict</code> function</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb74-2" data-line-number="2"><span class="co"># Construct current prediction</span></a>
<a class="sourceLine" id="cb74-3" data-line-number="3">alpha =<span class="st"> </span>bodyfat.lm<span class="op">$</span>coefficients[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">beta =<span class="st"> </span>bodyfat.lm<span class="op">$</span>coefficients[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb74-5" data-line-number="5">new_x =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(bodyfat<span class="op">$</span>Abdomen), <span class="kw">max</span>(bodyfat<span class="op">$</span>Abdomen), </a>
<a class="sourceLine" id="cb74-6" data-line-number="6">            <span class="dt">length.out =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb74-7" data-line-number="7">y_hat =<span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>new_x</a>
<a class="sourceLine" id="cb74-8" data-line-number="8"></a>
<a class="sourceLine" id="cb74-9" data-line-number="9"><span class="co"># Get lower and upper bounds for mean</span></a>
<a class="sourceLine" id="cb74-10" data-line-number="10">ymean =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(bodyfat.lm,</a>
<a class="sourceLine" id="cb74-11" data-line-number="11">                            <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Abdomen =</span> new_x),</a>
<a class="sourceLine" id="cb74-12" data-line-number="12">                            <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>,</a>
<a class="sourceLine" id="cb74-13" data-line-number="13">                            <span class="dt">level =</span> <span class="fl">0.95</span>))</a>
<a class="sourceLine" id="cb74-14" data-line-number="14"></a>
<a class="sourceLine" id="cb74-15" data-line-number="15"><span class="co"># Get lower and upper bounds for prediction</span></a>
<a class="sourceLine" id="cb74-16" data-line-number="16">ypred =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(bodyfat.lm,</a>
<a class="sourceLine" id="cb74-17" data-line-number="17">                          <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Abdomen =</span> new_x),</a>
<a class="sourceLine" id="cb74-18" data-line-number="18">                          <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>,</a>
<a class="sourceLine" id="cb74-19" data-line-number="19">                          <span class="dt">level =</span> <span class="fl">0.95</span>))</a>
<a class="sourceLine" id="cb74-20" data-line-number="20"></a>
<a class="sourceLine" id="cb74-21" data-line-number="21">output =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> new_x, <span class="dt">y_hat =</span> y_hat, <span class="dt">ymean_lwr =</span> ymean<span class="op">$</span>lwr, <span class="dt">ymean_upr =</span> ymean<span class="op">$</span>upr, </a>
<a class="sourceLine" id="cb74-22" data-line-number="22">                    <span class="dt">ypred_lwr =</span> ypred<span class="op">$</span>lwr, <span class="dt">ypred_upr =</span> ypred<span class="op">$</span>upr)</a>
<a class="sourceLine" id="cb74-23" data-line-number="23"></a>
<a class="sourceLine" id="cb74-24" data-line-number="24"><span class="co"># Extract potential outlier data point</span></a>
<a class="sourceLine" id="cb74-25" data-line-number="25">outlier =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> bodyfat<span class="op">$</span>Abdomen[<span class="dv">39</span>], <span class="dt">y =</span> bodyfat<span class="op">$</span>Bodyfat[<span class="dv">39</span>])</a>
<a class="sourceLine" id="cb74-26" data-line-number="26"></a>
<a class="sourceLine" id="cb74-27" data-line-number="27"><span class="co"># Scatter plot of original</span></a>
<a class="sourceLine" id="cb74-28" data-line-number="28">plot1 =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> bodyfat, <span class="kw">aes</span>(<span class="dt">x =</span> Abdomen, <span class="dt">y =</span> Bodyfat)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb74-29" data-line-number="29"></a>
<a class="sourceLine" id="cb74-30" data-line-number="30"><span class="co"># Add bounds of mean and prediction</span></a>
<a class="sourceLine" id="cb74-31" data-line-number="31">plot2 =<span class="st"> </span>plot1 <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-32" data-line-number="32"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> output, <span class="kw">aes</span>(<span class="dt">x =</span> new_x, <span class="dt">y =</span> y_hat, <span class="dt">color =</span> <span class="st">&quot;first&quot;</span>), <span class="dt">lty =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb74-33" data-line-number="33"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> output, <span class="kw">aes</span>(<span class="dt">x =</span> new_x, <span class="dt">y =</span> ymean_lwr, <span class="dt">lty =</span> <span class="st">&quot;second&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb74-34" data-line-number="34"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> output, <span class="kw">aes</span>(<span class="dt">x =</span> new_x, <span class="dt">y =</span> ymean_upr, <span class="dt">lty =</span> <span class="st">&quot;second&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb74-35" data-line-number="35"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> output, <span class="kw">aes</span>(<span class="dt">x =</span> new_x, <span class="dt">y =</span> ypred_upr, <span class="dt">lty =</span> <span class="st">&quot;third&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb74-36" data-line-number="36"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> output, <span class="kw">aes</span>(<span class="dt">x =</span> new_x, <span class="dt">y =</span> ypred_lwr, <span class="dt">lty =</span> <span class="st">&quot;third&quot;</span>)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-37" data-line-number="37"><span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>), <span class="dt">labels =</span> <span class="st">&quot;Posterior mean&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-38" data-line-number="38"><span class="st">  </span><span class="kw">scale_linetype_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;95% CI for mean&quot;</span>, <span class="st">&quot;95% CI for predictions&quot;</span>)</a>
<a class="sourceLine" id="cb74-39" data-line-number="39">                        , <span class="dt">name =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-40" data-line-number="40"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-41" data-line-number="41"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb74-42" data-line-number="42"></a>
<a class="sourceLine" id="cb74-43" data-line-number="43"><span class="co"># Identify potential outlier</span></a>
<a class="sourceLine" id="cb74-44" data-line-number="44">plot2 <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">data =</span> outlier, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y), <span class="dt">color =</span> <span class="st">&quot;orange&quot;</span>, <span class="dt">pch =</span> <span class="dv">1</span>, <span class="dt">cex =</span> <span class="dv">6</span>)</a></code></pre></div>
<p><img src="06-regression-01-Bayesian-simple-regression_files/figure-html/predict-intervals-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Note in the above plot, the legend “CI” can mean either confidence interval or credible interval. The difference comes down to the interpretation. For example, the prediction at the same abdominal circumference as in Case 39 is</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1">pred<span class="fl">.39</span> =<span class="st"> </span><span class="kw">predict</span>(bodyfat.lm, <span class="dt">newdata =</span> bodyfat[<span class="dv">39</span>, ], <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span>)</a>
<a class="sourceLine" id="cb75-2" data-line-number="2">out =<span class="st"> </span><span class="kw">cbind</span>(bodyfat[<span class="dv">39</span>,]<span class="op">$</span>Abdomen, pred<span class="fl">.39</span>)</a>
<a class="sourceLine" id="cb75-3" data-line-number="3"><span class="kw">colnames</span>(out) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;abdomen&quot;</span>, <span class="st">&quot;prediction&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>)</a>
<a class="sourceLine" id="cb75-4" data-line-number="4">out</a></code></pre></div>
<pre><code>##    abdomen prediction   lower    upper
## 39   148.1   54.21599 44.0967 64.33528</code></pre>
<p>Based on the data, a Bayesian would expect that a man with waist circumference of 148.1 centermeters should have bodyfat of 54.216% with 95% chance thta it is between 44.097% and 64.335%.</p>
<p>While we expect the majority of the data will be within the prediction intervals (the short dashed grey lines), Case 39 seems to be well below the interval. We next use Bayesian methods in Section <a href="introduction-to-bayesian-regression.html#sec:Checking-outliers">6.2</a> to calculate the probability that this case is abnormal or is an outlier by falling more than <span class="math inline">\(k\)</span> standard deviations from either side of the mean.</p>
</div>
<div id="sec:informative-prior" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Informative Priors</h3>
<p>Except from the noninformative reference prior, we may also consider using a more general semi-conjugate prior distribution of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> when there is information available about the parameters.</p>
<p>Since the data <span class="math inline">\(y_1,\cdots,y_n\)</span> are normally distributed, from Chapter 3 we see that a Normal-Gamma distribution will form a conjugacy in this situation. We then set up prior distributions through a hierarchical model. We first assume that, given <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> together follow the bivariate normal prior distribution, from which their marginal distributions are both normal,
<span class="math display">\[ 
\begin{aligned}
\alpha~|~\sigma^2 \sim &amp; \textsf{Normal}(a_0, \sigma^2\text{S}_\alpha) \\
\beta ~|~ \sigma^2 \sim &amp; \textsf{Normal}(b_0, \sigma^2\text{S}_\beta),
\end{aligned}
\]</span>
with covariance
<span class="math display">\[ \text{Cov}(\alpha, \beta ~|~\sigma^2) =\sigma^2 \text{S}_{\alpha\beta}. \]</span></p>
<p>Here, <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(S_\alpha\)</span>, <span class="math inline">\(S_\beta\)</span>, and <span class="math inline">\(S_{\alpha\beta}\)</span> are hyperparameters. This is equivalent to setting the coefficient vector <span class="math inline">\(\boldsymbol{\beta}= (\alpha, \beta)^T\)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> to have a bivariate normal distribution with convariance matrix <span class="math inline">\(\Sigma_0\)</span>
<span class="math display">\[ \Sigma_0 = \sigma^2\left(\begin{array}{cc} S_\alpha &amp; S_{\alpha\beta} \\
S_{\alpha\beta} &amp; S_\beta \end{array} \right). \]</span>
That is,
<span class="math display">\[ \boldsymbol{\beta}= (\alpha, \beta)^T ~|~\sigma^2 \sim \textsf{BivariateNormal}(\mathbf{b} = (a_0, b_0)^T, \sigma^2\Sigma_0). \]</span></p>
<p>Then for <span class="math inline">\(\sigma^2\)</span>, we will impose an inverse Gamma distribution as its prior distribution
<span class="math display">\[ 1/\sigma^2 \sim \textsf{Gamma}\left(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0}{2}\right). \]</span></p>
<p>Now the join prior distribution of <span class="math inline">\(\alpha, \beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> form a distribution that is analogous to the Normal-Gamma distribution. Prior information about <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are encoded in the hyperparameters <span class="math inline">\(a_0\)</span>, <span class="math inline">\(b_0\)</span>, <span class="math inline">\(\text{S}_\alpha\)</span>, <span class="math inline">\(\text{S}_\beta\)</span>, <span class="math inline">\(\text{S}_{\alpha\beta}\)</span>, <span class="math inline">\(\nu_0\)</span>, and <span class="math inline">\(\sigma_0\)</span>.</p>
<p>The marginal posterior distribution of the coefficient vector <span class="math inline">\(\boldsymbol{\beta}= (\alpha, \beta)\)</span> will be bivariate normal, and the marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> is again an inverse Gamma distribution
<span class="math display">\[ 1/\sigma^2~|~y_1,\cdots,y_n \sim \textsf{Gamma}\left(\frac{\nu_0+n}{2}, \frac{\nu_0\sigma_0^2+\text{SSE}}{2}\right). \]</span></p>
<p>One can see that the reference prior is the limiting case of this conjugate prior we impose. We usually use Gibbs sampling to approximate the joint posterior distribution instead of using the result directly, especially when we have more regression coefficients in multiple linear regression models. We omit the deviations of the posterior distributions due to the heavy use of advanced linear algebra. One can refer to <span class="citation">Hoff (<a href="#ref-hoff2009first">2009</a>)</span> for more details.</p>
<p>Based on any prior information we have for the model, we can also impose other priors and assumptions on <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> to get different Bayesian results. Most of these priors will not form any conjugacy and will require us to use simulation methods such as Markov Chain Monte Carlo (MCMC) for approximations. We will introduce the general idea of MCMC in Chapter 8.</p>
</div>
<div id="sec:derivations" class="section level3">
<h3><span class="header-section-number">6.1.4</span> (Optional) Derivations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma^2\)</span></h3>
<p>In this section, we will use the notations we introduced earlier such as <span class="math inline">\(\text{SSE}\)</span>, the sum of squares of errors, <span class="math inline">\(\hat{\sigma}^2\)</span>, the mean squared error, <span class="math inline">\(\text{S}_{xx}\)</span>, <span class="math inline">\(\text{se}_{\alpha}\)</span>, <span class="math inline">\(\text{se}_{\beta}\)</span> and so on to simplify our calculations.</p>
<p>We will also use the following quantities derived from the formula of <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\bar{y}\)</span>, <span class="math inline">\(\hat{\alpha}\)</span>, and <span class="math inline">\(\hat{\beta}\)</span>
<span class="math display">\[
\begin{aligned}
&amp; \sum_i^n (x_i-\bar{x}) = 0 \\
&amp; \sum_i^n (y_i-\bar{y}) = 0 \\
&amp; \sum_i^n (y_i - \hat{y}_i) = \sum_i^n (y_i - (\hat{\alpha} + \hat{\beta} x_i)) = 0\\
&amp; \sum_i^n (x_i-\bar{x})(y_i - \hat{y}_i) = \sum_i^n (x_i-\bar{x})(y_i-\bar{y}-\hat{\beta}(x_i-\bar{x})) = \sum_i^n (x_i-\bar{x})(y_i-\bar{y})-\hat{\beta}\sum_i^n(x_i-\bar{x})^2 = 0\\
&amp; \sum_i^n x_i^2 = \sum_i^n (x_i-\bar{x})^2 + n\bar{x}^2 = \text{S}_{xx}+n\bar{x}^2
\end{aligned}
\]</span></p>
<p>We first further simplify the numerator inside the exponential function in the formula of <span class="math inline">\(p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots,y_n)\)</span>:
<span class="math display">\[ 
\begin{aligned}
 &amp; \sum_i^n \left(y_i - \alpha - \beta x_i\right)^2 \\
 = &amp; \sum_i^n \left(y_i - \hat{\alpha} - \hat{\beta}x_i - (\alpha - \hat{\alpha}) - (\beta - \hat{\beta})x_i\right)^2 \\
= &amp; \sum_i^n \left(y_i - \hat{\alpha} - \hat{\beta}x_i\right)^2 + \sum_i^n (\alpha - \hat{\alpha})^2 + \sum_i^n (\beta-\hat{\beta})^2(x_i)^2 \\
  &amp; - 2\sum_i^n (\alpha - \hat{\alpha})(y_i-\hat{\alpha}-\hat{\beta}x_i) - 2\sum_i^n (\beta-\hat{\beta})(x_i)(y_i-\hat{\alpha}-\hat{\beta}x_i) + 2\sum_i^n(\alpha - \hat{\alpha})(\beta-\hat{\beta})(x_i)\\
= &amp; \text{SSE} + n(\alpha-\hat{\alpha})^2 + (\beta-\hat{\beta})^2\sum_i^n x_i^2 - 2(\alpha-\hat{\alpha})\sum_i^n (y_i-\hat{y}_i) -2(\beta-\hat{\beta})\sum_i^n x_i(y_i-\hat{y}_i)+2(\alpha-\hat{\alpha})(\beta-\hat{\beta})(n\bar{x})
\end{aligned}
\]</span></p>
<p>It is clear that
<span class="math display">\[ -2(\alpha-\hat{\alpha})\sum_i^n(y_i-\hat{y}_i) = 0 \]</span></p>
<p>And
<span class="math display">\[
\begin{aligned}
-2(\beta-\hat{\beta})\sum_i^n x_i(y_i-\hat{y}_i) = &amp; -2(\beta-\hat{\beta})\sum_i(x_i-\bar{x})(y_i-\hat{y}_i) - 2(\beta-\hat{\beta})\sum_i^n \bar{x}(y_i-\hat{y}_i) \\
= &amp; -2(\beta-\hat{\beta})\times 0 - 2(\beta-\hat{\beta})\bar{x}\sum_i^n(y_i-\hat{y}_i) = 0
\end{aligned}
\]</span></p>
<p>Finally, we use the quantity that <span class="math inline">\(\displaystyle \sum_i^n x_i^2 = \sum_i^n(x_i-\bar{x})^2+ n\bar{x}^2\)</span> to combine the terms <span class="math inline">\(n(\alpha-\hat{\alpha})^2\)</span>, <span class="math inline">\(2\displaystyle (\alpha-\hat{\alpha})(\beta-\hat{\beta})\sum_i^n x_i\)</span>, and <span class="math inline">\(\displaystyle (\beta-\hat{\beta})^2\sum_i^n x_i^2\)</span> together.</p>
<p><span class="math display">\[
\begin{aligned}
 &amp; \sum_i^n (y_i-\alpha-\beta x_i)^2 \\
 = &amp; \text{SSE} + n(\alpha-\hat{\alpha})^2 +(\beta-\hat{\beta})^2\sum_i^n (x_i-\bar{x})^2 + (\beta-\hat{\beta})^2 (n\bar{x}^2)  +2(\alpha-\hat{\alpha})(\beta-\hat{\beta})(n\bar{x})\\
= &amp; \text{SSE} + (\beta-\hat{\beta})^2\text{S}_{xx} + n\left[(\alpha-\hat{\alpha}) +(\beta-\hat{\beta})\bar{x}\right]^2
\end{aligned}
\]</span></p>
<p>Therefore, the posterior joint distribution of <span class="math inline">\(\alpha, \beta, \sigma^2\)</span> can be simplied as
<span class="math display">\[ 
\begin{aligned}
p^*(\alpha, \beta,\sigma^2 ~|~y_1,\cdots, y_n) \propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\sum_i(y_i - \alpha - \beta x_i)^2}{2\sigma^2}\right) \\
= &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE} + n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2 + (\beta - \hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2\sigma^2}\right)
\end{aligned}
\]</span></p>
</div>
<div id="marginal-posterior-distribution-of-beta" class="section level3">
<h3><span class="header-section-number">6.1.5</span> Marginal Posterior Distribution of <span class="math inline">\(\beta\)</span></h3>
<p>To get the marginal posterior distribution of <span class="math inline">\(\beta\)</span>, we need to integrate out <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma^2\)</span> from <span class="math inline">\(p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots,y_n)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
p^*(\beta ~|~y_1,\cdots,y_n) = &amp; \int_0^\infty \int_{-\infty}^\infty p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots, y_n)\, d\alpha\, d\sigma^2 \\
= &amp; \int_0^\infty \left(\int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE} + n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2+(\beta-\hat{\beta})\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\alpha\right)\, d\sigma^2\\
= &amp; \int_0^\infty p^*(\beta, \sigma^2~|~y_1,\cdots, y_n)\, d\sigma^2
\end{aligned}
\]</span></p>
<p>We first calculate the inside integral, which gives us the joint posterior distribution of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>
<span class="math display">\[
\begin{aligned}
&amp; p^*(\beta, \sigma^2~|~y_1,\cdots,y_n) \\
= &amp; \int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE}+n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\alpha\\
= &amp; \int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right) \exp\left(-\frac{n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2}{2\sigma^2}\right)\, d\alpha \\
= &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right) \int_{-\infty}^\infty \exp\left(-\frac{n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2}{2\sigma^2}\right)\, d\alpha
\end{aligned}
\]</span></p>
<p>Here,
<span class="math display">\[ \exp\left(-\frac{n(\alpha-\hat{\alpha}+(\beta - \hat{\beta})\bar{x})^2}{2\sigma^2}\right) \]</span>
can be viewed as part of a normal distribution of <span class="math inline">\(\alpha\)</span>, with mean <span class="math inline">\(\hat{\alpha}-(\beta-\hat{\beta})\bar{x}\)</span>, and variance <span class="math inline">\(\sigma^2/n\)</span>. Therefore, the integral from the last line above is proportional to <span class="math inline">\(\sqrt{\sigma^2/n}\)</span>. We get</p>
<p><span class="math display">\[
\begin{aligned}
p^*(\beta, \sigma^2~|~y_1,\cdots,y_n) 
\propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right) \times \sqrt{\frac{\sigma^2}{n}}\\
\propto &amp; \frac{1}{(\sigma^2)^{(n+1)/2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2\sigma^2}\right)
\end{aligned}
\]</span></p>
<p>We then integrate <span class="math inline">\(\sigma^2\)</span> out to get the marginal distribution of <span class="math inline">\(\beta\)</span>. Here we first perform change of variable and set <span class="math inline">\(\sigma^2 = \frac{1}{\phi}\)</span>. Then the integral becomes
<span class="math display">\[
\begin{aligned}
p^*(\beta~|~y_1,\cdots, y_n) \propto &amp; \int_0^\infty \frac{1}{(\sigma^2)^{(n+1)/2}}\exp\left(-\frac{\text{SSE} + (\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\sigma^2 \\
\propto &amp; \int_0^\infty \phi^{\frac{n-3}{2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2}\phi\right)\, d\phi\\
\propto &amp; \left(\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2}\right)^{-\frac{(n-2)+1}{2}}\int_0^\infty s^{\frac{n-3}{2}}e^{-s}\, ds
\end{aligned}
\]</span></p>
<p>Here we use another change of variable by setting <span class="math inline">\(\displaystyle s= \frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2}\phi\)</span>, and the fact that <span class="math inline">\(\displaystyle \int_0^\infty s^{(n-3)/2}e^{-s}\, ds\)</span> gives us the Gamma function <span class="math inline">\(\Gamma(n-2)\)</span>, which is a constant.</p>
<p>We can rewrite the last line from above to obtain the marginal posterior distribution of <span class="math inline">\(\beta\)</span>. This marginal distribution is the Student’s <span class="math inline">\(t\)</span>-distribution with degrees of freedom <span class="math inline">\(n-2\)</span>, center <span class="math inline">\(\hat{\beta}\)</span>, and scale parameter <span class="math inline">\(\displaystyle \frac{\hat{\sigma}^2}{\sum_i(x_i-\bar{x})^2}\)</span></p>
<p><span class="math display">\[ p^*(\beta~|~y_1,\cdots,y_n) \propto
 \left[1+\frac{1}{n-2}\frac{(\beta - \hat{\beta})^2}{\frac{\text{SSE}}{n-2}/(\sum_i (x_i-\bar{x})^2)}\right]^{-\frac{(n-2)+1}{2}} = \left[1 + \frac{1}{n-2}\frac{(\beta - \hat{\beta})^2}{\hat{\sigma}^2/(\sum_i (x_i-\bar{x})^2)}\right]^{-\frac{(n-2)+1}{2}},
\]</span></p>
<p>where <span class="math inline">\(\displaystyle \frac{\hat{\sigma}^2}{\sum_i (x_i-\bar{x})^2}\)</span> is exactly the square of the standard error of <span class="math inline">\(\hat{\beta}\)</span> from the frequentist OLS model.</p>
<p>To summarize, under the reference prior, the marginal posterior distribution of the slope of the Bayesian simple linear regression follows the Student’s <span class="math inline">\(t\)</span>-distribution
<span class="math display">\[ 
\beta ~|~y_1,\cdots, y_n \sim \textsf{t}\left(n-2, \ \hat{\beta},\  \left(\text{se}_{\beta}\right)^2\right) 
\]</span></p>
</div>
<div id="marginal-posterior-distribution-of-alpha" class="section level3">
<h3><span class="header-section-number">6.1.6</span> Marginal Posterior Distribution of <span class="math inline">\(\alpha\)</span></h3>
<p>A similar approach will lead us to the marginal distribution of <span class="math inline">\(\alpha\)</span>. We again start from the joint posterior distribution
<span class="math display">\[ p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots,y_n) \propto \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE} + n(\alpha-\hat{\alpha}-(\beta-\hat{\beta})\bar{x})^2 + (\beta - \hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2\sigma^2}\right) \]</span></p>
<p>This time we integrate <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> out to get the marginal posterior distribution of <span class="math inline">\(\alpha\)</span>. We first compute the integral
<span class="math display">\[
\begin{aligned}
p^*(\alpha, \sigma^2~|~y_1,\cdots, y_n) = &amp; \int_{-\infty}^\infty p^*(\alpha, \beta, \sigma^2~|~y_1,\cdots, y_n)\, d\beta\\
= &amp; \int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE} + n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2 + (\beta - \hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2\sigma^2}\right)\, d\beta 
\end{aligned}
\]</span></p>
<p>Here we group the terms with <span class="math inline">\(\beta-\hat{\beta}\)</span> together, then complete the square so that we can treat is as part of a normal distribution function to simplify the integral
<span class="math display">\[
\begin{aligned}
&amp; n(\alpha-\hat{\alpha}+(\beta-\hat{\beta})\bar{x})^2+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2 \\
= &amp; (\beta-\hat{\beta})^2\left(\sum_i (x_i-\bar{x})^2 + n\bar{x}^2\right) + 2n\bar{x}(\alpha-\hat{\alpha})(\beta-\hat{\beta}) + n(\alpha-\hat{\alpha})^2 \\
= &amp; \left(\sum_i (x_i-\bar{x})^2 + n\bar{x}^2\right)\left[(\beta-\hat{\beta})+\frac{n\bar{x}(\alpha-\hat{\alpha})}{\sum_i(x_i-\bar{x})^2+n\bar{x}^2}\right]^2+ n(\alpha-\hat{\alpha})^2\left[\frac{\sum_i(x_i-\bar{x})^2}{\sum_i (x_i-\bar{x})^2+n\bar{x}^2}\right]\\
= &amp; \left(\sum_i (x_i-\bar{x})^2 + n\bar{x}^2\right)\left[(\beta-\hat{\beta})+\frac{n\bar{x}(\alpha-\hat{\alpha})}{\sum_i(x_i-\bar{x})^2+n\bar{x}^2}\right]^2+\frac{(\alpha-\hat{\alpha})^2}{\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2}}
\end{aligned}
\]</span></p>
<p>When integrating, we can then view
<span class="math display">\[ \exp\left(-\frac{\sum_i (x_i-\bar{x})^2+n\bar{x}^2}{2\sigma^2}\left(\beta-\hat{\beta}+\frac{n\bar{x}(\alpha-\hat{\alpha})}{\sum_i (x_i-\bar{x})^2+n\bar{x}^2}\right)^2\right) \]</span>
as part of a normal distribution function, and get
<span class="math display">\[
\begin{aligned}
&amp; p^*(\alpha, \sigma^2~|~y_1,\cdots,y_n) \\
\propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSE}+(\alpha-\hat{\alpha})^2/(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2})}{2\sigma^2}\right)\\
&amp; \times\int_{-\infty}^\infty \exp\left(-\frac{\sum_i (x_i-\bar{x})^2+n\bar{x}^2}{2\sigma^2}\left(\beta-\hat{\beta}+\frac{n\bar{x}(\alpha-\hat{\alpha})}{\sum_i (x_i-\bar{x})^2+n\bar{x}^2}\right)^2\right)\, d\beta \\
\propto &amp; \frac{1}{(\sigma^2)^{(n+1)/2}}\exp\left(-\frac{\text{SSE}+(\alpha-\hat{\alpha})^2/(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2})}{2\sigma^2}\right)
\end{aligned}
\]</span></p>
<p>To get the marginal posterior distribution of <span class="math inline">\(\alpha\)</span>, we again integrate <span class="math inline">\(\sigma^2\)</span> out. using the same change of variable <span class="math inline">\(\displaystyle \sigma^2=\frac{1}{\phi}\)</span>, and <span class="math inline">\(s=\displaystyle \frac{\text{SSE}+(\alpha-\hat{\alpha})^2/(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2})}{2}\phi\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; p^*(\alpha~|~y_1,\cdots,y_n) \\
= &amp; \int_0^\infty p^*(\alpha, \sigma^2~|~y_1,\cdots, y_n)\, d\sigma^2 \\
\propto &amp; \int_0^\infty \phi^{(n-3)/2}\exp\left(-\frac{\text{SSE}+(\alpha-\hat{\alpha})^2/(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2})}{2}\phi\right)\, d\phi\\
\propto &amp; \left(\text{SSE}+(\alpha-\hat{\alpha})^2/(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2})\right)^{-\frac{(n-2)+1}{2}}\int_0^\infty s^{(n-3)/2}e^{-s}\, ds\\
\propto &amp; \left[1+\frac{1}{n-2}\frac{(\alpha-\hat{\alpha})^2}{\frac{\text{SSE}}{n-2}\left(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2}\right)}\right]^{-\frac{(n-2)+1}{2}} = \left[1 + \frac{1}{n-2}\left(\frac{\alpha-\hat{\alpha}}{\text{se}_{\alpha}}\right)^2\right]^{-\frac{(n-2)+1}{2}}
\end{aligned}
\]</span></p>
<p>In the last line, we use the same trick as we did for <span class="math inline">\(\beta\)</span> to derive the form of the Student’s <span class="math inline">\(t\)</span>-distribution. This shows that the marginal posterior distribution of <span class="math inline">\(\alpha\)</span> also follows a Student’s <span class="math inline">\(t\)</span>-distribution, with <span class="math inline">\(n-2\)</span> degrees of freedom. Its center is <span class="math inline">\(\hat{\alpha}\)</span>, the estimate of
<span class="math inline">\(\alpha\)</span> in the frequentist OLS estimate, and its scale parameter is <span class="math inline">\(\displaystyle \hat{\sigma}^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i-\bar{x})^2}\right)\)</span>, which is the square of the standard error of <span class="math inline">\(\hat{\alpha}\)</span>.</p>
</div>
<div id="marginal-posterior-distribution-of-sigma2" class="section level3">
<h3><span class="header-section-number">6.1.7</span> Marginal Posterior Distribution of <span class="math inline">\(\sigma^2\)</span></h3>
<p>To show that the marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> follows the inverse Gamma distribution, we only need to show the precision <span class="math inline">\(\displaystyle \phi = \frac{1}{\sigma^2}\)</span> follows a Gamma distribution.</p>
<p>We have shown in Week 3 that taking the prior distribution of <span class="math inline">\(\sigma^2\)</span> proportional to <span class="math inline">\(\displaystyle \frac{1}{\sigma^2}\)</span> is equivalent to taking the prior distribution of <span class="math inline">\(\phi\)</span> proportional to <span class="math inline">\(\displaystyle \frac{1}{\phi}\)</span>
<span class="math display">\[ p(\sigma^2) \propto \frac{1}{\sigma^2}\qquad \Longrightarrow \qquad p(\phi)\propto \frac{1}{\phi} \]</span></p>
<p>Therefore, under the parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and the precision <span class="math inline">\(\phi\)</span>, we have the joint prior distribution as
<span class="math display">\[ p(\alpha, \beta, \phi) \propto \frac{1}{\phi} \]</span>
and the joint posterior distribution as
<span class="math display">\[ 
p^*(\alpha, \beta, \phi~|~y_1,\cdots,y_n) \propto \phi^{\frac{n}{2}-1}\exp\left(-\frac{\sum_i(y_i-\alpha-\beta x_i)}{2}\phi\right) 
\]</span></p>
<p>Using the partial results we have calculated previously, we get
<span class="math display">\[
p^*(\beta, \phi~|~y_1,\cdots,y_n) = \int_{-\infty}^\infty p^*(\alpha, \beta, \phi~|~y_1,\cdots,y_n)\, d\alpha \propto \phi^{\frac{n-3}{2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2}\phi\right) 
\]</span></p>
<p>Intergrating over <span class="math inline">\(\beta\)</span>, we finally have
<span class="math display">\[
\begin{aligned}
&amp; p^*(\phi~|~y_1,\cdots,y_n) \\
\propto &amp; \int_{-\infty}^\infty \phi^{\frac{n-3}{2}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2}\phi\right)\, d\beta\\
= &amp; \phi^{\frac{n-3}{2}}\exp\left(-\frac{\text{SSE}}{2}\phi\right)\int_{-\infty}^\infty \exp\left(-\frac{(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2}\phi\right)\, d\beta\\
\propto &amp; \phi^{\frac{n-4}{2}}\exp\left(-\frac{\text{SSE}}{2}\phi\right) = \phi^{\frac{n-2}{2}-1}\exp\left(-\frac{\text{SSE}}{2}\phi\right).
\end{aligned}
\]</span></p>
<p>This is a Gamma distribution with shape parameter <span class="math inline">\(\displaystyle \frac{n-2}{2}\)</span> and rate parameter <span class="math inline">\(\displaystyle \frac{\text{SSE}}{2}\)</span>. Therefore, the updated <span class="math inline">\(\sigma^2\)</span> follows the inverse Gamma distribution
<span class="math display">\[ \phi = 1/\sigma^2~|~y_1,\cdots,y_n \sim \textsf{Gamma}\left(\frac{n-2}{2}, \frac{\text{SSE}}{2}\right). \]</span>
That is,
<span class="math display">\[ p(\phi~|~\text{data}) \propto \phi^{\frac{n-2}{2}-1}\exp\left(-\frac{\text{SSE}}{2}\phi\right). \]</span></p>
</div>
<div id="joint-normal-gamma-posterior-distributions" class="section level3">
<h3><span class="header-section-number">6.1.8</span> Joint Normal-Gamma Posterior Distributions</h3>
<p>Recall that the joint posterior distribution of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[ p^*(\beta, \sigma^2~|~\text{data}) \propto \frac{1}{\sigma^{n+1}}\exp\left(-\frac{\text{SSE}+(\beta-\hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right). \]</span></p>
<p>If we rewrite this using precision <span class="math inline">\(\phi=1/\sigma^2\)</span>, we get the joint posterior distribution of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\phi\)</span> to be
<span class="math display">\[ p^*(\beta, \phi~|~\text{data}) \propto \phi^{\frac{n-2}{2}}\exp\left(-\frac{\phi}{2}\left(\text{SSE}+(\beta-\hat{\beta})^2\sum_i (x_i-\bar{x})^2\right)\right). \]</span>
This joint posterior distribution can be viewed as the product of the posterior distribution of <span class="math inline">\(\beta\)</span> conditioning on <span class="math inline">\(\phi\)</span> and the posterior distribution of <span class="math inline">\(\phi\)</span>,
<span class="math display">\[ \pi^*(\beta~|~\phi,\text{data}) \times \pi^*(\phi~|~\text{data}) \propto \left[\phi\exp\left(-\frac{\phi}{2}(\beta-\hat{\beta})^2\sum_i (x_i-\bar{x})^2\right)\right] \times \left[\phi^{\frac{n-2}{2}-1}\exp\left(-\frac{\text{SSE}}{2}\phi\right)\right]. \]</span>
The first term in the product is exactly the Normal distribution with mean <span class="math inline">\(\hat{\beta}\)</span> and standard deviation <span class="math inline">\(\displaystyle \frac{\sigma^2}{\sum_i(x_i-\bar{x})^2} = \frac{\sigma^2}{\text{S}_{xx}}\)</span></p>
<p><span class="math display">\[ \beta ~|~\sigma^2,\ \text{data}~ \sim ~ \textsf{Normal}\left(\hat{\beta},\ \frac{\sigma^2}{\text{S}_{xx}}\right). \]</span>
The second term, is the Gamma distribution of the precision <span class="math inline">\(\phi\)</span>, or the inverse Gamma distribution of the variance <span class="math inline">\(\sigma^2\)</span>
<span class="math display">\[ 1/\sigma^2~|~\text{data}~\sim~\textsf{Gamma}\left(\frac{n-2}{2},\frac{\text{SSE}}{2}\right).\]</span></p>
<p>This means, the join posterior distribution of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>, under the reference prior, is a Normal-Gamma distribution. Similarly, the joint posterior distribution of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma^2\)</span> is also a Normal-Gamma distribution.
<span class="math display">\[ \alpha~|~\sigma^2, \text{data} ~\sim~\textsf{Normal}\left(\hat{\alpha}, \sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right)\right),\qquad \qquad 1/\sigma^2~|~\text{data}~\sim~ \textsf{Gamma}\left(\frac{n-2}{2}, \frac{\text{SSE}}{2}\right). \]</span></p>
<p>In fact, when we impose the bivariate normal distribution on <span class="math inline">\(\boldsymbol{\beta}= (\alpha, \beta)^T\)</span>, and inverse Gamma distribution on <span class="math inline">\(\sigma^2\)</span>, as we have discussed in Section <a href="introduction-to-bayesian-regression.html#sec:informative-prior">6.1.3</a>, the joint posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span> is a Normal-Gamma distribution. Since the reference prior is just the limiting case of this informative prior, it is not surprising that we will also get the limiting case Normal-Gamma distribution for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>

</div>
</div>
<div id="sec:Checking-outliers" class="section level2">
<h2><span class="header-section-number">6.2</span> Checking Outliers</h2>
<p>The plot and predictive intervals suggest that predictions for Case 39 are not well captured by the model. There is always the possibility that this case does not meet the assumptions of the simple linear regression model (wrong mean or variance) or could be in error. Model diagnostics such as plots of residuals versus fitted values are useful in identifying potential outliers. Now with the interpretation of Bayesian paradigm, we can go further to calculate the probability to demonstrate whether a case falls too far from the mean.</p>
<p>The article by <span class="citation">Chaloner and Brant (<a href="#ref-chaloner1988bayesian">1988</a>)</span> suggested an approach for defining outliers and then calculating the probability that a case or multiple cases were outliers, based on the posterior information of all observations. The assumed model for our simple linear regression is <span class="math inline">\(y_i=\alpha + \beta x_i+\epsilon_i\)</span>, with <span class="math inline">\(\epsilon_i\)</span> having independent, identical distributions that are normal with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>, i.e., <span class="math inline">\(\epsilon_i \mathrel{\mathop{\sim}\limits^{\rm iid}}\textsf{Normal}(0, \sigma^2)\)</span>. Chaloner &amp; Brant considered outliers to be points where the error or the model discrepancy <span class="math inline">\(\epsilon_i\)</span> is greater than <span class="math inline">\(k\)</span> standard deviations for some large <span class="math inline">\(k\)</span>, and then proceed to calculate the posterior probability that a case <span class="math inline">\(j\)</span> is an outlier to be
<span class="math display" id="eq:outlier-prob">\[\begin{equation} 
P(|\epsilon_j| &gt; k\sigma ~|~\text{data})
\tag{6.2}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\epsilon_j = y_j - \alpha-\beta x_j\)</span>, this is equivalent to calculating
<span class="math display">\[ P(|y_j-\alpha-\beta x_j| &gt; k\sigma~|~\text{data}).\]</span></p>
<div id="posterior-distribution-of-epsilon_j-conditioning-on-sigma2" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Posterior Distribution of <span class="math inline">\(\epsilon_j\)</span> Conditioning On <span class="math inline">\(\sigma^2\)</span></h3>
<p>At the end of Section <a href="introduction-to-bayesian-regression.html#sec:simple-linear">6.1</a>, we have discussed the posterior distributions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. It turns out that under the reference prior, both posterior distrubtions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, conditioning on <span class="math inline">\(\sigma^2\)</span>, are both normal
<span class="math display">\[ 
\begin{aligned}
\alpha ~|~\sigma^2, \text{data}~ &amp; \sim ~ \textsf{Normal}\left(\hat{\alpha}, \sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{\text{S}_{xx}}\right)\right), \\
\beta ~|~ \sigma^2, \text{data}~ &amp;\sim ~\textsf{Normal}\left(\hat{\beta}, \frac{\sigma^2}{\text{S}_{xx}}\right).
\end{aligned}
\]</span>
Using this information, we can obtain the posterior distribution of any residual <span class="math inline">\(\epsilon_j = y_j-\alpha-\beta x_j\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display" id="eq:post-distribution">\[\begin{equation} 
\epsilon_j~|~\sigma^2, \text{data} ~\sim ~ \textsf{Normal}\left(y_j-\hat{\alpha}-\hat{\beta}x_j,\ \frac{\sigma^2\sum_i(x_i-x_j)^2}{n\text{S}_{xx}}\right).
\tag{6.3}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\hat{\alpha}+\hat{\beta}x_j\)</span> is exactly the fitted value <span class="math inline">\(\hat{y}_j\)</span>, the mean of this Normal distribution is <span class="math inline">\(y_j-\hat{y}_j=\hat{\epsilon}_j\)</span>, which is the residual under the OLS estimates of the <span class="math inline">\(j\)</span>th observation.</p>
<p>Using this posterior distribution and the property of conditional probability, we can calculate the probability that the error <span class="math inline">\(\epsilon_j\)</span> lies outside of <span class="math inline">\(k\)</span> standard deviation of the mean, defined in equation <a href="introduction-to-bayesian-regression.html#eq:outlier-prob">(6.2)</a></p>
<p><span class="math display" id="eq:total-prob">\[\begin{equation} 
P(|\epsilon_j|&gt;k\sigma~|~\text{data}) = \int_0^\infty P(|\epsilon_j|&gt;k\sigma~|~\sigma^2,\text{data})p(\sigma^2~|~\text{data})\, d\sigma^2.
\tag{6.4}
\end{equation}\]</span></p>
<p>The probability <span class="math inline">\(P(|\epsilon_j|&gt;k\sigma~|~\sigma^2, \text{data})\)</span> can be calculated using the posterior distribution of <span class="math inline">\(\epsilon_j\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span> <a href="introduction-to-bayesian-regression.html#eq:post-distribution">(6.3)</a>
<span class="math display">\[ P(|\epsilon_j|&gt;k\sigma~|~\sigma^2,\text{data}) = \int_{|\epsilon_j|&gt;k\sigma}p(\epsilon_j~|~\sigma^2, \text{data})\, d\epsilon_j = \int_{k\sigma}^\infty p(\epsilon_j~|~\sigma^2, \text{data})\, d\epsilon_j+\int_{-\infty}^{-k\sigma}p(\epsilon_j~|~\sigma^2, \text{data})\, d\epsilon_j. \]</span></p>
<p>Recall that <span class="math inline">\(p(\epsilon_j~|~\sigma^2, \text{data})\)</span> is just a Normal distribution with mean <span class="math inline">\(\hat{\epsilon}_j\)</span>, standard deviation <span class="math inline">\(\displaystyle s=\sigma\sqrt{\frac{\sum_i (x_i-x_j)^2}{n\text{S}_{xx}}}\)</span>, we can use the <span class="math inline">\(z\)</span>-score and <span class="math inline">\(z\)</span>-table to look for this number. Let
<span class="math display">\[ z^* = \frac{\epsilon_j-\hat{\epsilon}_j}{s}. \]</span></p>
<p>The first integral <span class="math inline">\(\displaystyle \int_{k\sigma}^\infty p(\epsilon_j~|~\sigma^2,\text{data})\, d\epsilon_j\)</span> is equivalent to the probability
<span class="math display">\[ P\left(z^* &gt; \frac{k\sigma - \hat{\epsilon}_j}{s}\right) = P\left(z^*&gt; \frac{k\sigma-\hat{\epsilon}_j}{\sigma\sqrt{\sum_i(x_i-x_j)^2/\text{S}_{xx}}}\right) = P \left(z^* &gt; \frac{k-\hat{\epsilon}_j/\sigma}{\sqrt{\sum_i(x_i-x_j)^2/\text{S}_{xx}}}\right). \]</span>
That is the upper tail of the area under the standard Normal distribution when <span class="math inline">\(z^*\)</span> is larger than the critical value <span class="math inline">\(\displaystyle \frac{k-\hat{\epsilon}_j/\sigma}{\sqrt{\sum_i(x_i-x_j)^2/\text{S}_{xx}}}.\)</span></p>
<p>The second integral, <span class="math inline">\(\displaystyle \int_{-\infty}^{-k\sigma} p(\epsilon_j~|~\sigma^2, \text{data}\, d\epsilon_j\)</span>, is the same as the probability
<span class="math display">\[ P\left(z^* &lt; \frac{-k-\hat{\epsilon}_j/\sigma}{\sqrt{\sum_i(x_i-x_j)^2/\text{S}_{xx}}}\right), \]</span>
which is the lower tail of the area under the standard Normal distribution when <span class="math inline">\(z^*\)</span> is smaller than the critical value <span class="math inline">\(\displaystyle \frac{-k-\hat{\epsilon}_j/\sigma}{\sqrt{\sum_i(x_i-x_j)^2/\text{S}_{xx}}}.\)</span></p>
<p>After obtaining the two probabilities, we can move on to calculate the probability <span class="math inline">\(P(|\epsilon_j|&gt;k\sigma~|~\text{data})\)</span> using the formula given by <a href="introduction-to-bayesian-regression.html#eq:total-prob">(6.4)</a>. Since manual calculation is complicated, we often use numerical integration functions provided in R to finish the final integral.</p>
</div>
<div id="implementation-using-bas-package" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Implementation Using <code>BAS</code> Package</h3>
<p>The code for calculating the probability of outliers involves integration. We have implemented this in the function <code>Bayes.outlier</code> from the <code>BAS</code> package. This function takes an <code>lm</code> object and the value of <code>k</code> as arguments. Applying this to the <code>bodyfat</code> data for Case 39, we get</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="co"># Load `BAS` library and data. Run linear regression as in Section 6.1</span></a>
<a class="sourceLine" id="cb77-2" data-line-number="2"><span class="kw">library</span>(BAS)</a>
<a class="sourceLine" id="cb77-3" data-line-number="3"><span class="kw">data</span>(bodyfat)</a>
<a class="sourceLine" id="cb77-4" data-line-number="4">bodyfat.lm =<span class="st"> </span><span class="kw">lm</span>(Bodyfat <span class="op">~</span><span class="st"> </span>Abdomen, <span class="dt">data =</span> bodyfat)</a>
<a class="sourceLine" id="cb77-5" data-line-number="5"></a>
<a class="sourceLine" id="cb77-6" data-line-number="6"><span class="co">#</span></a>
<a class="sourceLine" id="cb77-7" data-line-number="7">outliers =<span class="st"> </span><span class="kw">Bayes.outlier</span>(bodyfat.lm, <span class="dt">k=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb77-8" data-line-number="8"></a>
<a class="sourceLine" id="cb77-9" data-line-number="9"><span class="co"># Extract the probability that Case 39 is an outlier</span></a>
<a class="sourceLine" id="cb77-10" data-line-number="10">prob<span class="fl">.39</span> =<span class="st"> </span>outliers<span class="op">$</span>prob.outlier[<span class="dv">39</span>]</a>
<a class="sourceLine" id="cb77-11" data-line-number="11">prob<span class="fl">.39</span></a></code></pre></div>
<pre><code>## [1] 0.9916833</code></pre>
<p>We see that this case has an extremely high probability of 0.992 of being more an outlier, that is, the error is greater than <span class="math inline">\(k=3\)</span> standard deviations, based on the fitted model and data.</p>
<p>With <span class="math inline">\(k=3\)</span>, however, there may be a high probability a priori of at least one outlier in a large sample. Let <span class="math inline">\(p = P(\text{any error $\epsilon_j$ lies within 3 standard deviations}) = P(\text{observation $j$ is not a outlier})\)</span>. Since we assume the prior distribution of <span class="math inline">\(\epsilon_j\)</span> is normal, we can calculate <span class="math inline">\(p\)</span> using the <code>pnorm</code> function. Let <span class="math inline">\(\Phi(z)\)</span> be the cumulative distribution of the standard Normal distribution, that is,
<span class="math display">\[ \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)\, dx. \]</span></p>
<p>Then <span class="math inline">\(p = 1-2\Phi(-k) = 1 - 2\Phi(-3)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Since we assume <span class="math inline">\(\epsilon_j\)</span> is independent, that the probability of no outlier is just the <span class="math inline">\(n\)</span>th power of <span class="math inline">\(p\)</span>. The event of getting at least 1 outlier is the complement of the event of getting no outliers. Therefore, the probability of getting at least 1 outlier is
<span class="math display">\[ P(\text{at least 1 outlier}) = 1 - P(\text{no outlier}) = 1 - p^n = 1 - (1 - 2\Phi(-3))^n.\]</span></p>
<p>We can compute this in R using</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(bodyfat)</a>
<a class="sourceLine" id="cb79-2" data-line-number="2"><span class="co"># probability of no outliers if outliers have errors greater than 3 standard deviation</span></a>
<a class="sourceLine" id="cb79-3" data-line-number="3">prob =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">3</span>))) <span class="op">^</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb79-4" data-line-number="4">prob</a></code></pre></div>
<pre><code>## [1] 0.5059747</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="co"># probability of at least one outlier</span></a>
<a class="sourceLine" id="cb81-2" data-line-number="2">prob.least1 =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">3</span>))) <span class="op">^</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb81-3" data-line-number="3">prob.least1</a></code></pre></div>
<pre><code>## [1] 0.4940253</code></pre>
<p>With <span class="math inline">\(n=252\)</span>, the probability of at least one outlier is much larger than say the marginal probability that one point is an outlier of 0.05. So we would expect that there will be at least one point where the error is more than 3 standard deviations from zero almost 50% of the time. Rather than fixing <span class="math inline">\(k\)</span>, we can fix the prior probability of no outliers <span class="math inline">\(P(\text{no outlier}) = 1 - p^n\)</span> to be say 0.95, and back solve the value of <span class="math inline">\(k\)</span> using the <code>qnorm</code> function</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1">new_k =<span class="st"> </span><span class="kw">qnorm</span>(<span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.95</span> <span class="op">^</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb83-2" data-line-number="2">new_k</a></code></pre></div>
<pre><code>## [1] 3.714602</code></pre>
<p>This leads to a larger value of <span class="math inline">\(k\)</span>. After adjusting <span class="math inline">\(k\)</span> the prior probability of no outliers is 0.95, we examine Case 39 again under this <span class="math inline">\(k\)</span></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># Calculate probability of being outliers using new `k` value</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2">outliers.new =<span class="st"> </span><span class="kw">Bayes.outlier</span>(bodyfat.lm, <span class="dt">k =</span> new_k)</a>
<a class="sourceLine" id="cb85-3" data-line-number="3"></a>
<a class="sourceLine" id="cb85-4" data-line-number="4"><span class="co"># Extract the probability of Case 39</span></a>
<a class="sourceLine" id="cb85-5" data-line-number="5">prob.new<span class="fl">.39</span> =<span class="st"> </span>outliers.new<span class="op">$</span>prob.outlier[<span class="dv">39</span>]</a>
<a class="sourceLine" id="cb85-6" data-line-number="6">prob.new<span class="fl">.39</span></a></code></pre></div>
<pre><code>## [1] 0.6847509</code></pre>
<p>The posterior probability of Case 39 being an outlier is about 0.685. While this is not strikingly large, it is much larger than the marginal prior probability of for a value lying about 3.7<span class="math inline">\(\sigma\)</span> away from 0, if we assume the error <span class="math inline">\(\epsilon_j\)</span> is normally distributed with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span>new_k)</a></code></pre></div>
<pre><code>## [1] 0.0002035241</code></pre>
<p>There is a substantial probability that Case 39 is an outlier. If you do view it as an outlier, what are your options? One option is to investigate the case and determine if the data are input incorrectly, and fix it. Another option is when you cannot confirm there is a data entry error, you may delete the observation from the analysis and refit the model without the case. If you do take this option, be sure to describe what you did so that your research is reproducible. You may want to apply diagnostics and calculate the probability of a case being an outlier using this reduced data. As a word of caution, if you discover that there are a large number of points that appear to be outliers, take a second look at your model assumptions, since the problem may be with the model rather than the data! A third option we will talk about later, is to combine inference under the model that retains this case as part of the population, and the model that treats it as coming from another population. This approach incorporates our uncertainty about whether the case is an outlier given the data.</p>
<p>The code of <code>Bayes.outlier</code> function is based on using a <strong>reference prior</strong> for the linear model and extends to multiple regression.</p>

</div>
</div>
<div id="sec:Bayes-multiple-regression" class="section level2">
<h2><span class="header-section-number">6.3</span> Bayesian Multiple Linear Regression</h2>
<p>In this section, we will discuss Bayesian inference in multiple linear regression. We will use the reference prior to provide the default or base line analysis of the model, which provides the correspondence between Bayesian and frequentist approaches.</p>
<div id="the-model" class="section level3">
<h3><span class="header-section-number">6.3.1</span> The Model</h3>
<p>To illustrate the idea, we use the data set on kid’s cognitive scores that we examined earlier. We predicted the value of the kid’s cognitive score from the mother’s high school status, mother’s IQ score, whether or not the mother worked during the first three years of the kid’s life, and the mother’s age. We set up the model as follows</p>
<p><span class="math display" id="eq:multi-model1">\[\begin{equation}
y_{\text{score},i} = \alpha + \beta_1 x_{\text{hs},i} + \beta_2 x_{\text{IQ},i} + \beta_3x_{\text{work},i} + \beta_4 x_{\text{age},i} + \epsilon_i, \quad i = 1,\cdots, n.
\tag{6.5}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(y_{\text{score},i}\)</span> is the <span class="math inline">\(i\)</span>th kid’s cognitive score. <span class="math inline">\(x_{\text{hs},i}\)</span>, <span class="math inline">\(x_{\text{IQ},i}\)</span>, <span class="math inline">\(x_{\text{work},i}\)</span>, and <span class="math inline">\(x_{\text{age},i}\)</span> represent the high school status, the IQ score, the work status during the first three years of the kid’s life, and the age of the <span class="math inline">\(i\)</span>th kid’s mother. <span class="math inline">\(\epsilon_i\)</span> is the error term. <span class="math inline">\(n\)</span> denotes the number of observations in this data set.</p>
<p>For better analyses, one usually centers the variable, which ends up getting the following form</p>
<p><span class="math display" id="eq:multi-model2">\[\begin{equation} 
y_{\text{score}, i} = \beta_0 + \beta_1 (x_{\text{hs},i}-\bar{x}_{\text{hs}}) + \beta_2 (x_{\text{IQ},i}-\bar{x}_{\text{IQ}}) + \beta_3(x_{\text{work},i}-\bar{x}_{\text{work}}) + \beta_4 (x_{\text{age},i}-\bar{x}_{\text{age}}) + \epsilon_i.
\tag{6.6}
\end{equation}\]</span></p>
<p>Under this tranformation, the coefficients, <span class="math inline">\(\beta_1,\ \beta_2,\ \beta_3\)</span>, <span class="math inline">\(\beta_4\)</span>, that are in front of the variables, are unchanged compared to the ones in <a href="introduction-to-bayesian-regression.html#eq:multi-model1">(6.5)</a>. However, the constant coefficient <span class="math inline">\(\beta_0\)</span> is no longer the constant coefficient <span class="math inline">\(\alpha\)</span> in <a href="introduction-to-bayesian-regression.html#eq:multi-model1">(6.5)</a>. Instead, under the assumption that <span class="math inline">\(\epsilon_i\)</span> is independently, identiacally normal, <span class="math inline">\(\beta_0\)</span> is the sample mean of the response variable <span class="math inline">\(Y_{\text{score}}\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> This provides more meaning to <span class="math inline">\(\beta_0\)</span>. Moreover, it is more convenient to use this “centered” model to derive analyses. The R codes in the <code>BAS</code> package are based on the form <a href="introduction-to-bayesian-regression.html#eq:multi-model2">(6.6)</a>.</p>
</div>
<div id="data-pre-processing" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Data Pre-processing</h3>
<p>We can download the data set from Gelman’s website and read the summary information of the data set using the <code>read.dta</code> function in the <code>foreign</code> package.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1"><span class="kw">library</span>(foreign)</a>
<a class="sourceLine" id="cb89-2" data-line-number="2">cognitive =<span class="st"> </span><span class="kw">read.dta</span>(<span class="st">&quot;http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta&quot;</span>)</a>
<a class="sourceLine" id="cb89-3" data-line-number="3"><span class="kw">summary</span>(cognitive)</a></code></pre></div>
<pre><code>##    kid_score         mom_hs           mom_iq          mom_work    
##  Min.   : 20.0   Min.   :0.0000   Min.   : 71.04   Min.   :1.000  
##  1st Qu.: 74.0   1st Qu.:1.0000   1st Qu.: 88.66   1st Qu.:2.000  
##  Median : 90.0   Median :1.0000   Median : 97.92   Median :3.000  
##  Mean   : 86.8   Mean   :0.7857   Mean   :100.00   Mean   :2.896  
##  3rd Qu.:102.0   3rd Qu.:1.0000   3rd Qu.:110.27   3rd Qu.:4.000  
##  Max.   :144.0   Max.   :1.0000   Max.   :138.89   Max.   :4.000  
##     mom_age     
##  Min.   :17.00  
##  1st Qu.:21.00  
##  Median :23.00  
##  Mean   :22.79  
##  3rd Qu.:25.00  
##  Max.   :29.00</code></pre>
<p>From the summary statistics, variables <code>mom_hs</code> and <code>mom_work</code> should be considered as categorical variables. We transform them into indicator variables where <code>mom_work = 1</code> if the mother worked for 1 or more years, and <code>mom_hs = 1</code> indicates the mother had more than a high school education.</p>
<p>The code is as below:<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">cognitive<span class="op">$</span>mom_work =<span class="st"> </span><span class="kw">as.numeric</span>(cognitive<span class="op">$</span>mom_work <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2">cognitive<span class="op">$</span>mom_hs =<span class="st"> </span><span class="kw">as.numeric</span>(cognitive<span class="op">$</span>mom_hs <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb91-3" data-line-number="3"></a>
<a class="sourceLine" id="cb91-4" data-line-number="4"><span class="co"># Modify column names of the data set</span></a>
<a class="sourceLine" id="cb91-5" data-line-number="5"><span class="kw">colnames</span>(cognitive) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;kid_score&quot;</span>, <span class="st">&quot;hs&quot;</span>, <span class="st">&quot;IQ&quot;</span>, <span class="st">&quot;work&quot;</span>, <span class="st">&quot;age&quot;</span>)</a></code></pre></div>
</div>
<div id="specify-bayesian-prior-distributions" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Specify Bayesian Prior Distributions</h3>
<p>For Bayesian inference, we need to specify a prior distribution for the error term <span class="math inline">\(\epsilon_i\)</span>. Since each kid’s cognitive score <span class="math inline">\(y_{\text{score},i}\)</span> is continuous, we assume that <span class="math inline">\(\epsilon_i\)</span> is independent, and identically distributed with the Normal distribution
<span class="math display">\[ \epsilon_i \mathrel{\mathop{\sim}\limits^{\rm iid}}\textsf{Normal}(0, \sigma^2), \]</span>
where <span class="math inline">\(\sigma^2\)</span> is the commonly shared variance of all observations.</p>
<p>We will also need to specify the prior distributions for all the coefficients <span class="math inline">\(\beta_0,\ \beta_1,\ \beta_2,\ \beta_3\)</span>, and <span class="math inline">\(\beta_4\)</span>. An informative prior, which assumes that the <span class="math inline">\(\beta\)</span>’s follow the multivariate normal distribution with covariance matrix <span class="math inline">\(\sigma^2\Sigma_0\)</span> can be used. We may further impose the inverse Gamma distribution to <span class="math inline">\(\sigma^2\)</span>, to complete the hierachical model
<span class="math display">\[ 
\begin{aligned}
\beta_0, \beta_1, \beta_2, \beta_3, \beta_4 ~|~\sigma^2 ~\sim ~ &amp; \textsf{Normal}((b_0, b_1, b_2, b_3, b_4)^T, \sigma^2\Sigma_0)\\
1/\sigma^2 \ ~\sim ~&amp; \textsf{Gamma}(\nu_0/2, \nu_0\sigma_0^2/2) 
\end{aligned}
\]</span></p>
<p>This gives us the multivariate Normal-Gamma conjugate family, with hyperparameters <span class="math inline">\(b_0, b_1, b_2, b_3, b_4, \Sigma_0, \nu_0\)</span>, and <span class="math inline">\(\sigma_0^2\)</span>. For this prior, we will need to specify the values of all the hyperparameters. This elicitation can be quite involved, especially when we do not have enough prior information about the variances, covariances of the coefficients and other prior hyperparameters. Therefore, we are going to adopt the noninformative reference prior, which is a limiting case of this multivariate Normal-Gamma prior.</p>
<p>The reference prior in the multiple linear regression model is similar to the reference prior we used in the simple linear regression model. The prior distribution of all the coefficients <span class="math inline">\(\beta\)</span>’s conditioning on <span class="math inline">\(\sigma^2\)</span> is the uniform prior, and the prior of <span class="math inline">\(\sigma^2\)</span> is proportional to its reciprocal
<span class="math display">\[ p(\beta_0,\beta_1,\beta_2,\beta_3,\beta_4~|~\sigma^2) \propto 1,\qquad\quad p(\sigma^2) \propto \frac{1}{\sigma^2}. \]</span></p>
<p>Under this reference prior, the marginal posterior distributions of the coefficients, <span class="math inline">\(\beta\)</span>’s, are parallel to the ones in simple linear regression. The marginal posterior distribution of <span class="math inline">\(\beta_j\)</span> is the Student’s <span class="math inline">\(t\)</span>-distributions with centers given by the frequentist OLS estimates <span class="math inline">\(\hat{\beta}_j\)</span>, scale parameter given by the standard error <span class="math inline">\((\text{se}_{\beta_j})^2\)</span> obtained from the OLS estimates
<span class="math display">\[
\beta_j~|~y_1,\cdots,y_n ~\sim ~\textsf{t}(n-p-1,\ \hat{\beta}_j,\ (\text{se}_{\beta_j})^2),\qquad j = 0, 1, \cdots, p.
\]</span></p>
<p>The degree of freedom of these <span class="math inline">\(t\)</span>-distributions is <span class="math inline">\(n-p-1\)</span>, where <span class="math inline">\(p\)</span> is the number of predictor variables. In the kid’s cognitive score example, <span class="math inline">\(p=4\)</span>. The posterior mean, <span class="math inline">\(\hat{\beta}_j\)</span>, is the center of the <span class="math inline">\(t\)</span>-distribution of <span class="math inline">\(\beta_j\)</span>, which is the same as the OLS estimates of <span class="math inline">\(\beta_j\)</span>. The posterior standard deviation of <span class="math inline">\(\beta_j\)</span>, which is the square root of the scale parameter of the <span class="math inline">\(t\)</span>-distribution, is <span class="math inline">\(\text{se}_{\beta_j}\)</span>, the standard error of <span class="math inline">\(\beta_j\)</span> under the OLS estimates. That means, under the reference prior, we can easily obtain the posterior mean and posterior standard deviation from using the <code>lm</code> function, since they are numerically equivalent to the counterpart of the frequentist approach.</p>
</div>
<div id="fitting-the-bayesian-model" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Fitting the Bayesian Model</h3>
<p>To gain more flexibility in choosing priors, we will instead use the <code>bas.lm</code> function in the <code>BAS</code> library, which allows us to specify different model priors and coefficient priors.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="co"># Import library</span></a>
<a class="sourceLine" id="cb92-2" data-line-number="2"><span class="kw">library</span>(BAS)</a>
<a class="sourceLine" id="cb92-3" data-line-number="3"></a>
<a class="sourceLine" id="cb92-4" data-line-number="4"><span class="co"># Use `bas.lm` to run regression model</span></a>
<a class="sourceLine" id="cb92-5" data-line-number="5">cog.bas =<span class="st"> </span><span class="kw">bas.lm</span>(kid_score <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> cognitive, <span class="dt">prior =</span> <span class="st">&quot;BIC&quot;</span>, </a>
<a class="sourceLine" id="cb92-6" data-line-number="6">                 <span class="dt">modelprior =</span> <span class="kw">Bernoulli</span>(<span class="dv">1</span>), <span class="dt">bestmodel =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">5</span>), <span class="dt">n.models =</span> <span class="dv">1</span>)</a></code></pre></div>
<p>The above <code>bas.lm</code> function uses the model formula the same as in the <code>lm</code>. It first specifies the response and predictor variables, a data argument to provide the data frame. The addition arguments further include the prior on the coefficients. We use <code>&quot;BIC&quot;</code> here to indicate that the model is based on the non-informative reference prior. (We will explain in the later section why we use the name <code>&quot;BIC&quot;</code>.) Since we will only provide one model, which is the one that includes all variables, we place all model prior probability to this exact model. This is specified in the <code>modelprior = Bernoulli(1)</code> argument. Because we want to fit using all variables, we use <code>bestmodel = rep(1,5)</code> to indicate that the intercept and all 4 predictors are included. The argument <code>n.models = 1</code> fits just this one model.</p>
</div>
<div id="posterior-means-and-posterior-standard-deviations" class="section level3">
<h3><span class="header-section-number">6.3.5</span> Posterior Means and Posterior Standard Deviations</h3>
<p>Similar to the OLS regression process, we can extract the posterior means and standard deviations of the coefficients using the <code>coef</code> function</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1">cog.coef =<span class="st"> </span><span class="kw">coef</span>(cog.bas)</a>
<a class="sourceLine" id="cb93-2" data-line-number="2">cog.coef</a></code></pre></div>
<pre><code>## 
##  Marginal Posterior Summaries of Coefficients: 
## 
##  Using  BMA 
## 
##  Based on the top  1 models 
##            post mean  post SD   post p(B != 0)
## Intercept  86.79724    0.87092   1.00000      
## hs          5.09482    2.31450   1.00000      
## IQ          0.56147    0.06064   1.00000      
## work        2.53718    2.35067   1.00000      
## age         0.21802    0.33074   1.00000</code></pre>
<p>From the last column in this summary, we see that the probability of the coefficients to be non-zero is always 1. This is because we specify the argument <code>bestmodel = rep(1, 5)</code> to force the model to include all variables. Notice on the first row we have the statistics of the <code>Intercept</code> <span class="math inline">\(\beta_0\)</span>. The posterior mean of <span class="math inline">\(\beta_0\)</span> is 86.8, which is completely different from the original <span class="math inline">\(y\)</span>-intercept of this model under the frequentist OLS regression. As we have stated previously, we consider the “centered” model under the Bayesian framework. Under this “centered” model and the reference prior, the posterior mean of the <code>Intercept</code> <span class="math inline">\(\beta_0\)</span> is now the sample mean of the response variable <span class="math inline">\(Y_{\text{score}}\)</span>.</p>
<p>We can visualize the coefficients <span class="math inline">\(\beta_1,\ \beta_2,\ \beta_3,\ \beta_4\)</span> using the <code>plot</code> function. We use the <code>subset</code> argument to plot only the coefficients of the predictors.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">col.lab =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="dt">col.axis =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</a>
<a class="sourceLine" id="cb95-2" data-line-number="2"><span class="kw">plot</span>(cog.coef, <span class="dt">subset =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">ask =</span> F)</a></code></pre></div>
<p><img src="06-regression-03-Bayesian-multi-regression_files/figure-html/plot-coef-1.png" width="672" /></p>
<p>These distributions all center at their respetive OLS estimates <span class="math inline">\(\hat{\beta}_j\)</span>, with the spread of the distribution related to the standard errors <span class="math inline">\(\text{se}_{\beta_j}\)</span>.</p>
</div>
<div id="credible-intervals-summary" class="section level3">
<h3><span class="header-section-number">6.3.6</span> Credible Intervals Summary</h3>
<p>We can also report the posterior means, posterior standard deviations, and the 95% credible intervals of the coefficients of all 4 predictors, which may give a clearer and more useful summary. The <code>BAS</code> library provides the method <code>confint</code> to extract the credible intervals from the output <code>cog.coef</code>. If we are only interested in the distributions of the coefficients of the 4 predictors, we may use the <code>parm</code> argument to restrict the variables shown in the summary</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="kw">confint</span>(cog.coef, <span class="dt">parm =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>)</a></code></pre></div>
<pre><code>##            2.5%     97.5%      beta
## hs    0.5456507 9.6439990 5.0948248
## IQ    0.4422784 0.6806616 0.5614700
## work -2.0830879 7.1574454 2.5371788
## age  -0.4320547 0.8680925 0.2180189
## attr(,&quot;Probability&quot;)
## [1] 0.95
## attr(,&quot;class&quot;)
## [1] &quot;confint.bas&quot;</code></pre>
<p>All together, we can generate a summary table showing the posterior means, posterior standard deviations, the upper and lower bounds of the 95% credible intervals of all coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2, \beta_3\)</span>, and <span class="math inline">\(\beta_4\)</span>.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">out =<span class="st"> </span><span class="kw">confint</span>(cog.coef)[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]  </a>
<a class="sourceLine" id="cb98-2" data-line-number="2"></a>
<a class="sourceLine" id="cb98-3" data-line-number="3"><span class="co"># Extract the upper and lower bounds of the credible intervals</span></a>
<a class="sourceLine" id="cb98-4" data-line-number="4">names =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;posterior mean&quot;</span>, <span class="st">&quot;posterior std&quot;</span>, <span class="kw">colnames</span>(out))</a>
<a class="sourceLine" id="cb98-5" data-line-number="5">out =<span class="st"> </span><span class="kw">cbind</span>(cog.coef<span class="op">$</span>postmean, cog.coef<span class="op">$</span>postsd, out)</a>
<a class="sourceLine" id="cb98-6" data-line-number="6"><span class="kw">colnames</span>(out) =<span class="st"> </span>names</a>
<a class="sourceLine" id="cb98-7" data-line-number="7"></a>
<a class="sourceLine" id="cb98-8" data-line-number="8"><span class="kw">round</span>(out, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##           posterior mean posterior std  2.5% 97.5%
## Intercept          86.80          0.87 85.09 88.51
## hs                  5.09          2.31  0.55  9.64
## IQ                  0.56          0.06  0.44  0.68
## work                2.54          2.35 -2.08  7.16
## age                 0.22          0.33 -0.43  0.87</code></pre>
<p>As in the simple linear aggression, the posterior estimates from the reference prior, that are in the table, are <strong>equivalent to the numbers</strong> reported from the <code>lm</code> function in R, or using the confident function in the OLS estimates. These intervals are centered at the posterior mean <span class="math inline">\(\hat{\beta}_j\)</span> with width given by the appropriate <span class="math inline">\(t\)</span> quantile with <span class="math inline">\(n-p-1\)</span> degrees of freedom times the posterior standard deviation <span class="math inline">\(\text{se}_{\beta_j}\)</span>. <strong>The primary difference is the interpretation of the intervals</strong>. For example, given this data, we believe there is a 95% chance that the kid’s cognitive score increases by 0.44 to 0.68 with one additional increase of the mother’s IQ score. The mother’s high school status has a larger effect where we believe that there is a 95% chance the kid would score of 0.55 up to 9.64 points higher if the mother had three or more years of high school. The credible intervals of the predictors <code>work</code> and <code>age</code> include 0, which implies that we may improve this model so that the model will accomplish a desired level of explanation or prediction with fewer predictors. We will explore model selection using Bayesian information criterion in the next chapter.</p>
</div>
</div>
<div id="summary-2" class="section level2">
<h2><span class="header-section-number">6.4</span> Summary</h2>
<p>We have provided Bayesian analyses for both simple linear regression and multiple linear regression using the default reference prior. We have seen that, under this reference prior, the marginal posterior distribution of the coefficients is the Student’s <span class="math inline">\(t\)</span>-distribution. Therefore, the posterior mean and posterior standard deviation of any coefficients are numerically equivalent to the corresponding frequentist OLS estimate and the standard error. This has provided us a base line analysis of Bayesian approach, which we can extend later when we introduce more different coefficient priors.</p>
<p>The difference is the interpretation. Since we have obtained the distribution of each coefficient, we can construct the credible interval, which provides us the probability that a specific coefficient falls into this credible interval.</p>
<p>We have also used the posterior distribution to analyze the probability of a particular observation being an outlier. We defined such probabiilty to be the probability that the error term is <span class="math inline">\(k\)</span> standard deviations away from 0. This probability is based on information of all data, instead of just the observation itself.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-chaloner1988bayesian">
<p>Chaloner, Kathryn, and Rollin Brant. 1988. “A Bayesian Approach to Outlier Detection and Residual Analysis.” <em>Biometrika</em> 75 (4). Oxford University Press: 651–59.</p>
</div>
<div id="ref-hoff2009first">
<p>Hoff, Peter D. 2009. <em>A First Course in Bayesian Statistical Methods</em>. Springer Science &amp; Business Media.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><span class="math inline">\((\alpha, \beta)^T\)</span> means we transpose the row vector <span class="math inline">\((\alpha, \beta)\)</span> into a column vector <span class="math inline">\(\left(\begin{array}{c} \alpha \\ \beta \end{array}\right)\)</span>.<a href="introduction-to-bayesian-regression.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(\Phi(-k)\)</span> actually represents the area of the lower tail under the standard Normal distribution curve <span class="math inline">\(k\)</span> standard deviations away from the mean 0.<a href="introduction-to-bayesian-regression.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Under the normal assumption, the mean of the error is 0. Taking mean on both sides of equation <a href="introduction-to-bayesian-regression.html#eq:multi-model2">(6.6)</a> immediately gives <span class="math inline">\(\beta_0=\bar{y}_{\text{score}}\)</span>.<a href="introduction-to-bayesian-regression.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Note: <code>as.numeric</code> is not necessary here. We use <code>as.numeric</code> to keep the names of the levels of the two variables short.<a href="introduction-to-bayesian-regression.html#fnref4" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing-with-normal-populations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-model-choice.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/statswithr/book/edit/master/06-regression-00-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
